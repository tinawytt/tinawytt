{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "av9RCfswtnhN",
    "outputId": "d350c391-7977-4cc5-d1d0-b9a5e5a6235a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn==0.4.2 in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall scikit-learn\n",
    "# !pip3 install scikit-learn==0.24.2\n",
    "# !pip3 uninstall imbalanced-learn==0.5.0\n",
    "!pip3 install imbalanced-learn==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydCZ2vYmsmfO",
    "outputId": "781d0afc-3254-4a90-a749-79a4c4d6df31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trixi in /usr/local/lib/python3.7/dist-packages (0.1.2.2)\n",
      "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.16.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.10.0+cu111)\n",
      "Requirement already satisfied: scikit-learn==0.20.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.4.1)\n",
      "Requirement already satisfied: python-telegram-bot>=10.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (13.11)\n",
      "Requirement already satisfied: pathos>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.2.8)\n",
      "Requirement already satisfied: portalocker>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.21.5)\n",
      "Requirement already satisfied: visdom>=0.1.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.1.8.9)\n",
      "Requirement already satisfied: graphviz>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (3.2.2)\n",
      "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.3.0)\n",
      "Requirement already satisfied: plotly>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (5.5.0)\n",
      "Requirement already satisfied: Flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.1.4)\n",
      "Requirement already satisfied: Pillow>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (9.0.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.1+cu111)\n",
      "Requirement already satisfied: slackclient>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.9.3)\n",
      "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.2)\n",
      "Requirement already satisfied: tb-nightly==1.14.0a20190523 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.14.0a20190523)\n",
      "Requirement already satisfied: umap-learn>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.4.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.17.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.1)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (7.1.2)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.12.2->trixi) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (1.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (1.6.6.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=2.5.1->trixi) (8.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (6.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2021.10.8)\n",
      "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (3.6.3)\n",
      "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2018.9)\n",
      "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (57.4.0)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (1.5.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.8.1->trixi) (1.3.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>3.5.2 in /usr/local/lib/python3.7/dist-packages (from slackclient>=1.3.1->trixi) (3.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.0.11)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.7.2)\n",
      "Requirement already satisfied: numba!=0.47,>=0.46 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.6->trixi) (0.51.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba!=0.47,>=0.46->umap-learn>=0.3.6->trixi) (0.34.0)\n",
      "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (0.1.0)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (2.23.0)\n",
      "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.32)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (22.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.10)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom>=0.1.8.4->trixi) (2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install trixi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2S22Nyy9-Ll"
   },
   "outputs": [],
   "source": [
    "from trixi.util.pytorchutils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHXu2WlBqJ3i",
    "outputId": "9f4bc06c-09a8-447e-907b-1d681d1393e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import random\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage.transform import resize\n",
    "from trixi.util.pytorchutils import set_seed\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(base_dir, pattern='*.npz', keys=None):\n",
    "    fls = []\n",
    "    files_len = []\n",
    "    dataset = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        i = 0\n",
    "        for filename in sorted(fnmatch.filter(files, pattern)):\n",
    "\n",
    "            if keys is not None and filename[:-4] in keys:\n",
    "                npz_file = os.path.join(root, filename)\n",
    "                numpy_array = np.load(npz_file)['data']\n",
    "                \n",
    "                fls.append(npz_file)\n",
    "                files_len.append(numpy_array.shape[1])\n",
    "\n",
    "                dataset.extend([i])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    return fls, files_len, dataset\n",
    "\n",
    "class SlimDataLoaderBase(object):\n",
    "    def __init__(self, data, batch_size, number_of_threads_in_multithreaded=None):\n",
    "        __metaclass__ = ABCMeta\n",
    "        self.number_of_threads_in_multithreaded = number_of_threads_in_multithreaded\n",
    "        self._data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.thread_id = 0\n",
    "\n",
    "    def set_thread_id(self, thread_id):\n",
    "        self.thread_id = thread_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.generate_train_batch()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_train_batch(self):\n",
    "        '''override this\n",
    "        Generate your batch from self._data .Make sure you generate the correct batch size (self.BATCH_SIZE)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class NumpyDataLoader(SlimDataLoaderBase):\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000,\n",
    "                 seed=None, file_pattern='*.npz', label=1, input=(0,), keys=None):\n",
    "\n",
    "        shorter_keys=[]\n",
    "        for key in keys:\n",
    "            arr=key.split('/')\n",
    "            \n",
    "            shorter_keys.append(arr[len(arr)-1])\n",
    "        \n",
    "        keys=shorter_keys\n",
    "        self.files, self.file_len, self.dataset = load_dataset(base_dir=base_dir, pattern=file_pattern, keys=keys )\n",
    "        \n",
    "        super(NumpyDataLoader, self).__init__(self.dataset, batch_size, num_batches)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_next = False\n",
    "        if mode == \"train\":\n",
    "            self.use_next = False\n",
    "\n",
    "        self.idxs = list(range(0, len(self.dataset)))\n",
    "\n",
    "        self.data_len = len(self.dataset)\n",
    "\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "        if isinstance(label, int):\n",
    "            label = (label,)\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "\n",
    "        self.np_data = np.asarray(self.dataset)\n",
    "\n",
    "    def reshuffle(self):\n",
    "        print(\"Reshuffle...\")\n",
    "        random.shuffle(self.idxs)\n",
    "        print(\"Initializing... this might take a while...\")\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        open_arr = random.sample(self._data, self.batch_size)\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        n_items = min(self.data_len // self.batch_size, self.num_batches)\n",
    "        return n_items\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idxs = self.idxs\n",
    "        data_len = len(self.dataset)\n",
    "        np_data = self.np_data\n",
    "\n",
    "        if item > len(self):\n",
    "            raise StopIteration()\n",
    "        if (item * self.batch_size) == data_len:\n",
    "            raise StopIteration()\n",
    "\n",
    "        start_idx = (item * self.batch_size) % data_len\n",
    "        stop_idx = ((item + 1) * self.batch_size) % data_len\n",
    "\n",
    "        if ((item + 1) * self.batch_size) == data_len:\n",
    "            stop_idx = data_len\n",
    "\n",
    "        if stop_idx > start_idx:\n",
    "            idxs = idxs[start_idx:stop_idx]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "        open_arr = np_data[idxs]\n",
    "\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def get_data_from_array(self, open_array):\n",
    "        data = []\n",
    "        fnames = []\n",
    "        idxs = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in open_array:\n",
    "            fn_name = self.files[idx]\n",
    "\n",
    "            numpy_array = np.load(fn_name)\n",
    "\n",
    "            data.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            if self.label is not None:\n",
    "                labels.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            fnames.append(self.files[idx])\n",
    "            idxs.append(idx)\n",
    "\n",
    "        ret_dict = {'data': data, 'fnames': fnames, 'idxs': idxs}\n",
    "        if self.label is not None:\n",
    "            ret_dict['seg'] = labels\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "class WrappedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.is_indexable = False\n",
    "        if hasattr(self.dataset, \"__getitem__\") and not (hasattr(self.dataset, \"use_next\") and self.dataset.use_next is True):\n",
    "            self.is_indexable = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if not self.is_indexable:\n",
    "            item = next(self.dataset)\n",
    "        else:\n",
    "            item = self.dataset[index]\n",
    "        # item = self.transform(**item)\n",
    "        print(type(item))\n",
    "        old_data=item['data']\n",
    "        old_seg=item['seg']\n",
    "        \n",
    "        new_shape=(128,128,128)\n",
    "        result_list=[]\n",
    "        \n",
    "        for i in range(len(old_data)):\n",
    "            result_element = np.zeros(new_shape, dtype=old_data[i].dtype)\n",
    "            result_element= resize(old_data[i].astype(float), new_shape, order=3, clip=True, anti_aliasing=False)\n",
    "            result_list.append(result_element)\n",
    "        item['data']=result_list\n",
    "        result_list=[]\n",
    "        result_element = np.zeros(new_shape, dtype=old_seg[0].dtype)\n",
    "        unique_labels = np.unique(old_seg[0])\n",
    "        for i, c in enumerate(unique_labels):\n",
    "            mask = old_seg[0] == c\n",
    "            reshaped_multihot = resize(mask.astype(float), new_shape, order=1, mode=\"edge\", clip=True, anti_aliasing=False)\n",
    "            result_element[reshaped_multihot >= 0.5] = c\n",
    "        \n",
    "        result_list.append(result_element)\n",
    "        item['seg']=result_list\n",
    "        print(np.unique(result_list[0]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.dataset.num_batches)\n",
    "\n",
    "\n",
    "class MultiThreadedDataLoader(object):\n",
    "    def __init__(self, data_loader,  num_processes,transform=None, **kwargs):\n",
    "\n",
    "        self.cntr = 1\n",
    "        self.ds_wrapper = WrappedDataset(data_loader, transform)\n",
    "\n",
    "        self.generator = DataLoader(self.ds_wrapper, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                                    num_workers=num_processes, pin_memory=True, drop_last=False,\n",
    "                                    worker_init_fn=self.get_worker_init_fn())\n",
    "\n",
    "        self.num_processes = num_processes\n",
    "        self.iter = None\n",
    "\n",
    "    def get_worker_init_fn(self):\n",
    "        def init_fn(worker_id):\n",
    "            set_seed(worker_id + self.cntr)\n",
    "\n",
    "        return init_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.kill_iterator()\n",
    "        self.iter = iter(self.generator)\n",
    "        return self.iter\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter is None:\n",
    "            self.iter = iter(self.generator)\n",
    "        return next(self.iter)\n",
    "\n",
    "    def renew(self):\n",
    "        self.cntr += 1\n",
    "        self.kill_iterator()\n",
    "        self.generator.worker_init_fn = self.get_worker_init_fn()\n",
    "        self.iter = iter(self.generator)\n",
    "\n",
    "    def kill_iterator(self):\n",
    "        try:\n",
    "            if self.iter is not None:\n",
    "                self.iter._shutdown_workers()\n",
    "                for p in self.iter.workers:\n",
    "                    p.terminate()\n",
    "        except:\n",
    "            print(\"Could not kill Dataloader Iterator\")\n",
    "\n",
    "class NumpyDataSet(object):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000, seed=None, num_processes=8, num_cached_per_queue=8 * 4, target_size=128,\n",
    "                 file_pattern='*.npz', label=1, input=(0,), do_reshuffle=True, keys=None):#8*4->2*4  8->2\n",
    "\n",
    "        data_loader = NumpyDataLoader(base_dir=base_dir, mode=mode, batch_size=batch_size, num_batches=num_batches, seed=seed, file_pattern=file_pattern,\n",
    "                                      input=input, label=label, keys=keys)\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.do_reshuffle = do_reshuffle\n",
    "        self.number_of_slices = 1\n",
    "\n",
    "        self.transforms = None\n",
    "        self.augmenter = MultiThreadedDataLoader(data_loader, num_processes,num_cached_per_queue=num_cached_per_queue, seeds=seed,\n",
    "                                                 shuffle=do_reshuffle)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.do_reshuffle:\n",
    "            self.data_loader.reshuffle()\n",
    "        self.augmenter.renew()\n",
    "        return self.augmenter\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.augmenter)\n",
    "\n",
    "data_dir='/home/jovyan/main/BraTS2020_TrainingData/'\n",
    "with open(os.path.join(data_dir, \"splits.pkl\"), 'rb') as f:\n",
    "  splits = pickle.load(f)\n",
    "tr_keys = splits[0]['train']\n",
    "val_keys = splits[0]['val']\n",
    "test_keys = splits[0]['test']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data_loader = NumpyDataSet(data_dir, target_size=64, batch_size=8,keys=tr_keys)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 137, 167, 133)\n",
    "# (5, 143, 176, 131)\n",
    "# (5, 137, 167, 124)\n",
    "# (5, 143, 187, 138)\n",
    "# (5, 144, 170, 138)\n",
    "# (5, 140, 186, 136)\n",
    "# (5, 146, 160, 127)\n",
    "# (5, 139, 158, 137)\n",
    "# (5, 145, 172, 140)\n",
    "# (5, 140, 173, 130)\n",
    "# (5, 140, 164, 145)\n",
    "# (5, 140, 182, 132)\n",
    "# (5, 144, 168, 146)\n",
    "# (5, 141, 178, 135)\n",
    "# (5, 145, 177, 140)\n",
    "# (5, 147, 167, 125)\n",
    "# (5, 138, 167, 142)\n",
    "# (5, 146, 178, 139)\n",
    "# (5, 136, 157, 133)\n",
    "# (5, 140, 187, 137)\n",
    "# (5, 137, 174, 139)\n",
    "# (5, 137, 166, 140)\n",
    "# (5, 141, 177, 140)\n",
    "# (5, 137, 169, 138)\n",
    "# (5, 143, 174, 137)\n",
    "# (5, 141, 178, 140)\n",
    "# (5, 143, 187, 132)\n",
    "# (5, 141, 174, 138)\n",
    "# (5, 136, 173, 131)\n",
    "# (5, 136, 168, 134)\n",
    "# (5, 141, 171, 130)\n",
    "# (5, 135, 163, 129)\n",
    "# (5, 138, 168, 128)\n",
    "# (5, 149, 176, 143)\n",
    "# (5, 138, 179, 140)\n",
    "# (5, 138, 167, 135)\n",
    "# (5, 141, 176, 144)\n",
    "# (5, 134, 157, 126)\n",
    "# (5, 142, 184, 141)\n",
    "# (5, 129, 175, 128)\n",
    "# (5, 144, 170, 130)\n",
    "# (5, 144, 173, 137)\n",
    "# (5, 130, 167, 148)\n",
    "# (5, 135, 162, 142)\n",
    "# (5, 140, 176, 133)\n",
    "# (5, 142, 185, 132)\n",
    "# (5, 141, 165, 143)\n",
    "# (5, 141, 173, 131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
