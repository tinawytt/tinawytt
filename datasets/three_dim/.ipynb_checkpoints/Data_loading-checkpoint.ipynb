{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "av9RCfswtnhN",
    "outputId": "d350c391-7977-4cc5-d1d0-b9a5e5a6235a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn==0.4.2 in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall scikit-learn\n",
    "# !pip3 install scikit-learn==0.24.2\n",
    "# !pip3 uninstall imbalanced-learn==0.5.0\n",
    "!pip3 install imbalanced-learn==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ydCZ2vYmsmfO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "781d0afc-3254-4a90-a749-79a4c4d6df31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trixi in /usr/local/lib/python3.7/dist-packages (0.1.2.2)\n",
      "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.16.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.10.0+cu111)\n",
      "Requirement already satisfied: scikit-learn==0.20.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.4.1)\n",
      "Requirement already satisfied: python-telegram-bot>=10.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (13.11)\n",
      "Requirement already satisfied: pathos>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.2.8)\n",
      "Requirement already satisfied: portalocker>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.21.5)\n",
      "Requirement already satisfied: visdom>=0.1.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.1.8.9)\n",
      "Requirement already satisfied: graphviz>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (3.2.2)\n",
      "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.3.0)\n",
      "Requirement already satisfied: plotly>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (5.5.0)\n",
      "Requirement already satisfied: Flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.1.4)\n",
      "Requirement already satisfied: Pillow>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (9.0.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.1+cu111)\n",
      "Requirement already satisfied: slackclient>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.9.3)\n",
      "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.2)\n",
      "Requirement already satisfied: tb-nightly==1.14.0a20190523 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.14.0a20190523)\n",
      "Requirement already satisfied: umap-learn>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.4.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.17.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.1)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (7.1.2)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.12.2->trixi) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (1.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (1.6.6.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=2.5.1->trixi) (8.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (6.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2021.10.8)\n",
      "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (3.6.3)\n",
      "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2018.9)\n",
      "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (57.4.0)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (1.5.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.8.1->trixi) (1.3.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>3.5.2 in /usr/local/lib/python3.7/dist-packages (from slackclient>=1.3.1->trixi) (3.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.0.11)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.7.2)\n",
      "Requirement already satisfied: numba!=0.47,>=0.46 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.6->trixi) (0.51.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba!=0.47,>=0.46->umap-learn>=0.3.6->trixi) (0.34.0)\n",
      "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (0.1.0)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (2.23.0)\n",
      "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.32)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (22.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.10)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom>=0.1.8.4->trixi) (2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install trixi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "j2S22Nyy9-Ll"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/main/networks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0a20190523\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import version\n",
    "print(version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHXu2WlBqJ3i",
    "outputId": "9f4bc06c-09a8-447e-907b-1d681d1393e2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"author\": \"tinawytt\",\n",
      "    \"base_dir\": \"/home/jovyan/main/\",\n",
      "    \"batch_size\": 8,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_root_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_test_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"device\": \"cuda\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"in_channels\": 1,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"model_dir\": \"/home/jovyan/main/\",\n",
      "    \"n_epochs\": 10,\n",
      "    \"name\": \"Basic_Unet\",\n",
      "    \"num_classes\": 3,\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"split_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"start_visdom\": true\n",
      "}\n",
      "It's Alive!\n",
      "You can navigate to http://jupyter-yw016:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/visdom/server.py\", line 1803, in start_server\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 586, in run_forever\n",
      "    self._check_running()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 578, in _check_running\n",
      "    raise RuntimeError('This event loop is already running')\n",
      "RuntimeError: This event loop is already running\n",
      "Setting up a new session...\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/connection.py\", line 95, in create_connection\n",
      "    raise err\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 398, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 239, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1279, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1325, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1274, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1034, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 974, in send\n",
      "    self.connect()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 205, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe708458f40>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/adapters.py\", line 440, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 785, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /env/Basic_Unet (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe708458f40>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/visdom/__init__.py\", line 708, in _send\n",
      "    return self._handle_post(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 577, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /env/Basic_Unet (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe708458f40>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "[Errno 111] Connection refused\n",
      "on_close() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Visdom on Port: 8080\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53/982509777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m exp = UNetExperiment3D(config=c, name=c.name, n_epochs=c.n_epochs,\n\u001b[0m\u001b[1;32m    352\u001b[0m                          \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_rnd_to_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_rnd_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                          \u001b[0;31m# visdomlogger_kwargs={\"auto_start\": c.start_visdom},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/experiment/pytorchexperiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, name, n_epochs, seed, base_dir, globs, resume, ignore_resume_config, resume_save_types, resume_reset_epochs, parse_sys_argv, checkpoint_to_cpu, save_checkpoint_every_epoch, explogger_kwargs, explogger_freq, loggers, append_rnd_to_name)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mzip_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sources.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mSourcePacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Init objects in config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/util/sourcepacker.py\u001b[0m in \u001b[0;36mzip_sources\u001b[0;34m(globs, filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpy_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourcePacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_sources_and_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mrepo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourcePacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__file__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/util/sourcepacker.py\u001b[0m in \u001b[0;36mgit_info\u001b[0;34m(file_)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mold_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/posixpath.py\u001b[0m in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;34m\"\"\"Return an absolute path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import random\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage.transform import resize\n",
    "from trixi.util.pytorchutils import set_seed\n",
    "import numpy as np\n",
    "import pickle\n",
    "from UNet3D import UNet3D\n",
    "\n",
    "# from networks.UNet3D import UNet3D\n",
    "from trixi.util import Config\n",
    "from trixi.experiment.pytorchexperiment import PytorchExperiment\n",
    "\n",
    "def get_config():\n",
    "    # Set your own path, if needed.\n",
    "    data_root_dir = '/home/jovyan/main/BraTS2020_TrainingData/'  # The path where the downloaded dataset is stored.\n",
    "\n",
    "    c = Config(\n",
    "        update_from_argv=True,  # If set 'True', it allows to update each configuration by a cmd/terminal parameter.\n",
    "\n",
    "        # Train parameters\n",
    "        num_classes=3,\n",
    "        in_channels=1,\n",
    "        batch_size=8,\n",
    "        patch_size=64,\n",
    "        n_epochs=10,\n",
    "        learning_rate=0.0002,\n",
    "        fold=0,  # The 'splits.pkl' may contain multiple folds. Here we choose which one we want to use.\n",
    "\n",
    "        device=\"cuda\",  # 'cuda' is the default CUDA device, you can use also 'cpu'. For more information, see https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "        # Logging parameters\n",
    "        name='Basic_Unet',\n",
    "        author='tinawytt',  # Author of this project\n",
    "        plot_freq=10,  # How often should stuff be shown in visdom\n",
    "        append_rnd_string=False,  # Appends a random string to the experiment name to make it unique.\n",
    "        start_visdom=True,  # You can either start a visom server manually or have trixi start it for you.\n",
    "\n",
    "        do_instancenorm=True,  # Defines whether or not the UNet does a instance normalization in the contracting path\n",
    "        do_load_checkpoint=False,\n",
    "        checkpoint_dir='',\n",
    "\n",
    "        \n",
    "        base_dir='/home/jovyan/main/',  # Where to log the output of the experiment.\n",
    "\n",
    "        data_root_dir=data_root_dir,  # The path where the downloaded dataset is stored.\n",
    "        data_dir=data_root_dir,  # This is where your training and validation data is stored\n",
    "        data_test_dir=data_root_dir,  # This is where your test data is stored\n",
    "\n",
    "        split_dir=data_root_dir,  # This is where the 'splits.pkl' file is located, that holds your splits.\n",
    "\n",
    "        # execute a segmentation process on a specific image using the model\n",
    "        model_dir=os.path.join('/home/jovyan/main/', ''),  # the model being used for segmentation\n",
    "    )\n",
    "\n",
    "    print(c)\n",
    "    return c\n",
    "\n",
    "def load_dataset(base_dir, pattern='*.npz', keys=None):\n",
    "    fls = []\n",
    "    files_len = []\n",
    "    dataset = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        i = 0\n",
    "        for filename in sorted(fnmatch.filter(files, pattern)):\n",
    "\n",
    "            if keys is not None and filename[:-4] in keys:\n",
    "                npz_file = os.path.join(root, filename)\n",
    "                numpy_array = np.load(npz_file)['data']\n",
    "                \n",
    "                fls.append(npz_file)\n",
    "                files_len.append(numpy_array.shape[1])\n",
    "\n",
    "                dataset.extend([i])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    return fls, files_len, dataset\n",
    "\n",
    "class SlimDataLoaderBase(object):\n",
    "    def __init__(self, data, batch_size, number_of_threads_in_multithreaded=None):\n",
    "        __metaclass__ = ABCMeta\n",
    "        self.number_of_threads_in_multithreaded = number_of_threads_in_multithreaded\n",
    "        self._data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.thread_id = 0\n",
    "\n",
    "    def set_thread_id(self, thread_id):\n",
    "        self.thread_id = thread_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.generate_train_batch()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_train_batch(self):\n",
    "        '''override this\n",
    "        Generate your batch from self._data .Make sure you generate the correct batch size (self.BATCH_SIZE)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class NumpyDataLoader(SlimDataLoaderBase):\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000,\n",
    "                 seed=None, file_pattern='*.npz', label=1, input=(0,), keys=None):\n",
    "\n",
    "        shorter_keys=[]\n",
    "        for key in keys:\n",
    "            arr=key.split('/')\n",
    "            \n",
    "            shorter_keys.append(arr[len(arr)-1])\n",
    "        \n",
    "        keys=shorter_keys\n",
    "        self.files, self.file_len, self.dataset = load_dataset(base_dir=base_dir, pattern=file_pattern, keys=keys )\n",
    "        \n",
    "        super(NumpyDataLoader, self).__init__(self.dataset, batch_size, num_batches)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_next = False\n",
    "        if mode == \"train\":\n",
    "            self.use_next = False\n",
    "\n",
    "        self.idxs = list(range(0, len(self.dataset)))\n",
    "\n",
    "        self.data_len = len(self.dataset)\n",
    "\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "        if isinstance(label, int):\n",
    "            label = (label,)\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "\n",
    "        self.np_data = np.asarray(self.dataset)\n",
    "\n",
    "    def reshuffle(self):\n",
    "        print(\"Reshuffle...\")\n",
    "        random.shuffle(self.idxs)\n",
    "        print(\"Initializing... this might take a while...\")\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        open_arr = random.sample(self._data, self.batch_size)\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        n_items = min(self.data_len // self.batch_size, self.num_batches)\n",
    "        return n_items\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idxs = self.idxs\n",
    "        data_len = len(self.dataset)\n",
    "        np_data = self.np_data\n",
    "\n",
    "        if item > len(self):\n",
    "            raise StopIteration()\n",
    "        if (item * self.batch_size) == data_len:\n",
    "            raise StopIteration()\n",
    "\n",
    "        start_idx = (item * self.batch_size) % data_len\n",
    "        stop_idx = ((item + 1) * self.batch_size) % data_len\n",
    "\n",
    "        if ((item + 1) * self.batch_size) == data_len:\n",
    "            stop_idx = data_len\n",
    "\n",
    "        if stop_idx > start_idx:\n",
    "            idxs = idxs[start_idx:stop_idx]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "        open_arr = np_data[idxs]\n",
    "\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def get_data_from_array(self, open_array):\n",
    "        data = []\n",
    "        fnames = []\n",
    "        idxs = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in open_array:\n",
    "            fn_name = self.files[idx]\n",
    "\n",
    "            numpy_array = np.load(fn_name)\n",
    "\n",
    "            data.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            if self.label is not None:\n",
    "                labels.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            fnames.append(self.files[idx])\n",
    "            idxs.append(idx)\n",
    "\n",
    "        ret_dict = {'data': data, 'fnames': fnames, 'idxs': idxs}\n",
    "        if self.label is not None:\n",
    "            ret_dict['seg'] = labels\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "class WrappedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.is_indexable = False\n",
    "        if hasattr(self.dataset, \"__getitem__\") and not (hasattr(self.dataset, \"use_next\") and self.dataset.use_next is True):\n",
    "            self.is_indexable = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if not self.is_indexable:\n",
    "            item = next(self.dataset)\n",
    "        else:\n",
    "            item = self.dataset[index]\n",
    "        # item = self.transform(**item)\n",
    "        print(type(item))\n",
    "        old_data=item['data']\n",
    "        old_seg=item['seg']\n",
    "        \n",
    "        new_shape=(128,128,128)\n",
    "        result_list=[]\n",
    "        \n",
    "        for i in range(len(old_data)):\n",
    "            result_element = np.zeros(new_shape, dtype=old_data[i].dtype)\n",
    "            result_element= resize(old_data[i].astype(float), new_shape, order=3, clip=True, anti_aliasing=False)\n",
    "            result_list.append(result_element)\n",
    "        item['data']=result_list\n",
    "        result_list=[]\n",
    "        result_element = np.zeros(new_shape, dtype=old_seg[0].dtype)\n",
    "        unique_labels = np.unique(old_seg[0])\n",
    "        for i, c in enumerate(unique_labels):\n",
    "            mask = old_seg[0] == c\n",
    "            reshaped_multihot = resize(mask.astype(float), new_shape, order=1, mode=\"edge\", clip=True, anti_aliasing=False)\n",
    "            result_element[reshaped_multihot >= 0.5] = c\n",
    "        \n",
    "        result_list.append(result_element)\n",
    "        item['seg']=result_list\n",
    "        print(np.unique(result_list[0]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.dataset.num_batches)\n",
    "\n",
    "\n",
    "class MultiThreadedDataLoader(object):\n",
    "    def __init__(self, data_loader,  num_processes,transform=None, **kwargs):\n",
    "\n",
    "        self.cntr = 1\n",
    "        self.ds_wrapper = WrappedDataset(data_loader, transform)\n",
    "\n",
    "        self.generator = DataLoader(self.ds_wrapper, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                                    num_workers=num_processes, pin_memory=True, drop_last=False,\n",
    "                                    worker_init_fn=self.get_worker_init_fn())\n",
    "\n",
    "        self.num_processes = num_processes\n",
    "        self.iter = None\n",
    "\n",
    "    def get_worker_init_fn(self):\n",
    "        def init_fn(worker_id):\n",
    "            set_seed(worker_id + self.cntr)\n",
    "\n",
    "        return init_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.kill_iterator()\n",
    "        self.iter = iter(self.generator)\n",
    "        return self.iter\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter is None:\n",
    "            self.iter = iter(self.generator)\n",
    "        return next(self.iter)\n",
    "\n",
    "    def renew(self):\n",
    "        self.cntr += 1\n",
    "        self.kill_iterator()\n",
    "        self.generator.worker_init_fn = self.get_worker_init_fn()\n",
    "        self.iter = iter(self.generator)\n",
    "\n",
    "    def kill_iterator(self):\n",
    "        try:\n",
    "            if self.iter is not None:\n",
    "                self.iter._shutdown_workers()\n",
    "                for p in self.iter.workers:\n",
    "                    p.terminate()\n",
    "        except:\n",
    "            print(\"Could not kill Dataloader Iterator\")\n",
    "\n",
    "class NumpyDataSet(object):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000, seed=None, num_processes=8, num_cached_per_queue=8 * 4, target_size=128,\n",
    "                 file_pattern='*.npz', label=1, input=(0,), do_reshuffle=True, keys=None):#8*4->2*4  8->2\n",
    "\n",
    "        data_loader = NumpyDataLoader(base_dir=base_dir, mode=mode, batch_size=batch_size, num_batches=num_batches, seed=seed, file_pattern=file_pattern,\n",
    "                                      input=input, label=label, keys=keys)\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.do_reshuffle = do_reshuffle\n",
    "        self.number_of_slices = 1\n",
    "\n",
    "        self.transforms = None\n",
    "        self.augmenter = MultiThreadedDataLoader(data_loader, num_processes,num_cached_per_queue=num_cached_per_queue, seeds=seed,\n",
    "                                                 shuffle=do_reshuffle)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.do_reshuffle:\n",
    "            self.data_loader.reshuffle()\n",
    "        self.augmenter.renew()\n",
    "        return self.augmenter\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.augmenter)\n",
    "\n",
    "class UNetExperiment3D(PytorchExperiment):\n",
    "    def setup(self):\n",
    "        data_dir='/home/jovyan/main/BraTS2020_TrainingData/'\n",
    "        with open(os.path.join(data_dir, \"splits.pkl\"), 'rb') as f:\n",
    "          splits = pickle.load(f)\n",
    "        tr_keys = splits[0]['train']\n",
    "        val_keys = splits[0]['val']\n",
    "        test_keys = splits[0]['test']\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_data_loader = NumpyDataSet(data_dir, target_size=64, batch_size=8,keys=tr_keys)\n",
    "        self.model = UNet3D(num_classes=3, in_channels=1)\n",
    "        self.model.to(self.device)\n",
    "        print(\"ok\")\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        pass\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n",
    "c = get_config()\n",
    "exp = UNetExperiment3D(config=c, name=c.name, n_epochs=c.n_epochs,\n",
    "                         seed=42, append_rnd_to_name=c.append_rnd_string, globs=globals(),\n",
    "                         # visdomlogger_kwargs={\"auto_start\": c.start_visdom},\n",
    "                         loggers={\n",
    "                             \"visdom\": (\"visdom\", {\"auto_start\": c.start_visdom})\n",
    "                         }\n",
    "                         )\n",
    "\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 137, 167, 133)\n",
    "# (5, 143, 176, 131)\n",
    "# (5, 137, 167, 124)\n",
    "# (5, 143, 187, 138)\n",
    "# (5, 144, 170, 138)\n",
    "# (5, 140, 186, 136)\n",
    "# (5, 146, 160, 127)\n",
    "# (5, 139, 158, 137)\n",
    "# (5, 145, 172, 140)\n",
    "# (5, 140, 173, 130)\n",
    "# (5, 140, 164, 145)\n",
    "# (5, 140, 182, 132)\n",
    "# (5, 144, 168, 146)\n",
    "# (5, 141, 178, 135)\n",
    "# (5, 145, 177, 140)\n",
    "# (5, 147, 167, 125)\n",
    "# (5, 138, 167, 142)\n",
    "# (5, 146, 178, 139)\n",
    "# (5, 136, 157, 133)\n",
    "# (5, 140, 187, 137)\n",
    "# (5, 137, 174, 139)\n",
    "# (5, 137, 166, 140)\n",
    "# (5, 141, 177, 140)\n",
    "# (5, 137, 169, 138)\n",
    "# (5, 143, 174, 137)\n",
    "# (5, 141, 178, 140)\n",
    "# (5, 143, 187, 132)\n",
    "# (5, 141, 174, 138)\n",
    "# (5, 136, 173, 131)\n",
    "# (5, 136, 168, 134)\n",
    "# (5, 141, 171, 130)\n",
    "# (5, 135, 163, 129)\n",
    "# (5, 138, 168, 128)\n",
    "# (5, 149, 176, 143)\n",
    "# (5, 138, 179, 140)\n",
    "# (5, 138, 167, 135)\n",
    "# (5, 141, 176, 144)\n",
    "# (5, 134, 157, 126)\n",
    "# (5, 142, 184, 141)\n",
    "# (5, 129, 175, 128)\n",
    "# (5, 144, 170, 130)\n",
    "# (5, 144, 173, 137)\n",
    "# (5, 130, 167, 148)\n",
    "# (5, 135, 162, 142)\n",
    "# (5, 140, 176, 133)\n",
    "# (5, 142, 185, 132)\n",
    "# (5, 141, 165, 143)\n",
    "# (5, 141, 173, 131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import pilutil\n",
    "\n",
    "x = np.zeros((255, 255), dtype=np.uint8)\n",
    "x[:] = np.arange(255)\n",
    "pilutil.imsave('gradient.png', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from slackclient import SlackClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
