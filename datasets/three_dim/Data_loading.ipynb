{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "av9RCfswtnhN",
    "outputId": "d350c391-7977-4cc5-d1d0-b9a5e5a6235a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn==0.4.2 in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall scikit-learn\n",
    "# !pip3 install scikit-learn==0.24.2\n",
    "# !pip3 uninstall imbalanced-learn==0.5.0\n",
    "!pip3 install imbalanced-learn==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ydCZ2vYmsmfO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "781d0afc-3254-4a90-a749-79a4c4d6df31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trixi in /usr/local/lib/python3.7/dist-packages (0.1.2.2)\n",
      "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.16.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.10.0+cu111)\n",
      "Requirement already satisfied: scikit-learn==0.20.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.4.1)\n",
      "Requirement already satisfied: python-telegram-bot>=10.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (13.11)\n",
      "Requirement already satisfied: pathos>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.2.8)\n",
      "Requirement already satisfied: portalocker>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.21.5)\n",
      "Requirement already satisfied: visdom>=0.1.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.1.8.9)\n",
      "Requirement already satisfied: graphviz>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (3.2.2)\n",
      "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.3.0)\n",
      "Requirement already satisfied: plotly>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (5.5.0)\n",
      "Requirement already satisfied: Flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.1.4)\n",
      "Requirement already satisfied: Pillow>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (9.0.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.1+cu111)\n",
      "Requirement already satisfied: slackclient>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.9.3)\n",
      "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.2)\n",
      "Requirement already satisfied: tb-nightly==1.14.0a20190523 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.14.0a20190523)\n",
      "Requirement already satisfied: umap-learn>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.4.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.17.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.1)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (7.1.2)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.12.2->trixi) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (1.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (1.6.6.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=2.5.1->trixi) (8.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (6.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2021.10.8)\n",
      "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (3.6.3)\n",
      "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2018.9)\n",
      "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (57.4.0)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (1.5.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.8.1->trixi) (1.3.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>3.5.2 in /usr/local/lib/python3.7/dist-packages (from slackclient>=1.3.1->trixi) (3.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.0.11)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.7.2)\n",
      "Requirement already satisfied: numba!=0.47,>=0.46 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.6->trixi) (0.51.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba!=0.47,>=0.46->umap-learn>=0.3.6->trixi) (0.34.0)\n",
      "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (0.1.0)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (2.23.0)\n",
      "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.32)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (22.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.10)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom>=0.1.8.4->trixi) (2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install trixi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j2S22Nyy9-Ll"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/main/networks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0a20190523\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import version\n",
    "print(version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHXu2WlBqJ3i",
    "outputId": "9f4bc06c-09a8-447e-907b-1d681d1393e2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"author\": \"tinawytt\",\n",
      "    \"base_dir\": \"/home/jovyan/main/\",\n",
      "    \"batch_size\": 8,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_root_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_test_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"device\": \"cuda\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"in_channels\": 1,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"model_dir\": \"/home/jovyan/main/\",\n",
      "    \"n_epochs\": 10,\n",
      "    \"name\": \"Basic_Unet\",\n",
      "    \"num_classes\": 3,\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"split_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"start_visdom\": true\n",
      "}\n",
      "None\n",
      "None\n",
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"author\": \"tinawytt\",\n",
      "    \"base_dir\": \"/home/jovyan/main/\",\n",
      "    \"batch_size\": 8,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_root_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_test_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"device\": \"cuda\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"in_channels\": 1,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"model_dir\": \"/home/jovyan/main/\",\n",
      "    \"n_epochs\": 10,\n",
      "    \"name\": \"Basic_Unet\",\n",
      "    \"num_classes\": 3,\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"seed\": 42,\n",
      "    \"split_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"start_visdom\": true\n",
      "}\n",
      "-----\n",
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"author\": \"tinawytt\",\n",
      "    \"base_dir\": \"/home/jovyan/main/\",\n",
      "    \"batch_size\": 8,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_root_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_test_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"device\": \"cuda\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"in_channels\": 1,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"model_dir\": \"/home/jovyan/main/\",\n",
      "    \"n_epochs\": 10,\n",
      "    \"name\": \"Basic_Unet\",\n",
      "    \"num_classes\": 3,\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"seed\": 42,\n",
      "    \"split_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"start_visdom\": true\n",
      "}\n",
      "None\n",
      "===\n",
      "/home/jovyan/main/20220304-103630_Basic_Unet\n",
      "/home/jovyan/main/20220304-103630_Basic_Unet/save\n",
      "/home/jovyan/main/20220304-103630_Basic_Unet/save\n",
      "<class 'dict'>\n",
      "None\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] config_path resume_path\n",
      "ipykernel_launcher.py: error: the following arguments are required: resume_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import random\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage.transform import resize\n",
    "from trixi.util.pytorchutils import set_seed\n",
    "import numpy as np\n",
    "import pickle\n",
    "from UNet3D import UNet3D\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import OrderedDict\n",
    "# from networks.UNet3D import UNet3D\n",
    "from trixi.util import Config\n",
    "from trixi.experiment.pytorchexperiment import PytorchExperiment\n",
    "from torch import nn\n",
    "from trixi.util.config import update_from_sys_argv\n",
    "from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\n",
    "from trixi.util import ResultLogDict, SourcePacker\n",
    "\n",
    "def get_config():\n",
    "    # Set your own path, if needed.\n",
    "    data_root_dir = '/home/jovyan/main/BraTS2020_TrainingData/'  # The path where the downloaded dataset is stored.\n",
    "\n",
    "    c = Config(\n",
    "        update_from_argv=True,  # If set 'True', it allows to update each configuration by a cmd/terminal parameter.\n",
    "\n",
    "        # Train parameters\n",
    "        num_classes=3,\n",
    "        in_channels=1,\n",
    "        batch_size=8,\n",
    "        patch_size=64,\n",
    "        n_epochs=10,\n",
    "        learning_rate=0.0002,\n",
    "        fold=0,  # The 'splits.pkl' may contain multiple folds. Here we choose which one we want to use.\n",
    "\n",
    "        device=\"cuda\",  # 'cuda' is the default CUDA device, you can use also 'cpu'. For more information, see https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "        # Logging parameters\n",
    "        name='Basic_Unet',\n",
    "        author='tinawytt',  # Author of this project\n",
    "        plot_freq=10,  # How often should stuff be shown in visdom\n",
    "        append_rnd_string=False,  # Appends a random string to the experiment name to make it unique.\n",
    "        start_visdom=True,  # You can either start a visom server manually or have trixi start it for you.\n",
    "\n",
    "        do_instancenorm=True,  # Defines whether or not the UNet does a instance normalization in the contracting path\n",
    "        do_load_checkpoint=False,\n",
    "        checkpoint_dir='',\n",
    "\n",
    "        \n",
    "        base_dir='/home/jovyan/main/',  # Where to log the output of the experiment.\n",
    "\n",
    "        data_root_dir=data_root_dir,  # The path where the downloaded dataset is stored.\n",
    "        data_dir=data_root_dir,  # This is where your training and validation data is stored\n",
    "        data_test_dir=data_root_dir,  # This is where your test data is stored\n",
    "\n",
    "        split_dir=data_root_dir,  # This is where the 'splits.pkl' file is located, that holds your splits.\n",
    "\n",
    "        # execute a segmentation process on a specific image using the model\n",
    "        model_dir=os.path.join('/home/jovyan/main/', ''),  # the model being used for segmentation\n",
    "    )\n",
    "\n",
    "    print(c)\n",
    "    return c\n",
    "\n",
    "def load_dataset(base_dir, pattern='*.npz', keys=None):\n",
    "    fls = []\n",
    "    files_len = []\n",
    "    dataset = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        i = 0\n",
    "        for filename in sorted(fnmatch.filter(files, pattern)):\n",
    "\n",
    "            if keys is not None and filename[:-4] in keys:\n",
    "                npz_file = os.path.join(root, filename)\n",
    "                numpy_array = np.load(npz_file)['data']\n",
    "                \n",
    "                fls.append(npz_file)\n",
    "                files_len.append(numpy_array.shape[1])\n",
    "\n",
    "                dataset.extend([i])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    return fls, files_len, dataset\n",
    "\n",
    "class SlimDataLoaderBase(object):\n",
    "    def __init__(self, data, batch_size, number_of_threads_in_multithreaded=None):\n",
    "        __metaclass__ = ABCMeta\n",
    "        self.number_of_threads_in_multithreaded = number_of_threads_in_multithreaded\n",
    "        self._data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.thread_id = 0\n",
    "\n",
    "    def set_thread_id(self, thread_id):\n",
    "        self.thread_id = thread_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.generate_train_batch()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_train_batch(self):\n",
    "        '''override this\n",
    "        Generate your batch from self._data .Make sure you generate the correct batch size (self.BATCH_SIZE)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class NumpyDataLoader(SlimDataLoaderBase):\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000,\n",
    "                 seed=None, file_pattern='*.npz', label=1, input=(0,), keys=None):\n",
    "\n",
    "        shorter_keys=[]\n",
    "        for key in keys:\n",
    "            arr=key.split('/')\n",
    "            \n",
    "            shorter_keys.append(arr[len(arr)-1])\n",
    "        \n",
    "        keys=shorter_keys\n",
    "        self.files, self.file_len, self.dataset = load_dataset(base_dir=base_dir, pattern=file_pattern, keys=keys )\n",
    "        \n",
    "        super(NumpyDataLoader, self).__init__(self.dataset, batch_size, num_batches)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_next = False\n",
    "        if mode == \"train\":\n",
    "            self.use_next = False\n",
    "\n",
    "        self.idxs = list(range(0, len(self.dataset)))\n",
    "\n",
    "        self.data_len = len(self.dataset)\n",
    "\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "        if isinstance(label, int):\n",
    "            label = (label,)\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "\n",
    "        self.np_data = np.asarray(self.dataset)\n",
    "\n",
    "    def reshuffle(self):\n",
    "        print(\"Reshuffle...\")\n",
    "        random.shuffle(self.idxs)\n",
    "        print(\"Initializing... this might take a while...\")\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        open_arr = random.sample(self._data, self.batch_size)\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        n_items = min(self.data_len // self.batch_size, self.num_batches)\n",
    "        return n_items\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idxs = self.idxs\n",
    "        data_len = len(self.dataset)\n",
    "        np_data = self.np_data\n",
    "\n",
    "        if item > len(self):\n",
    "            raise StopIteration()\n",
    "        if (item * self.batch_size) == data_len:\n",
    "            raise StopIteration()\n",
    "\n",
    "        start_idx = (item * self.batch_size) % data_len\n",
    "        stop_idx = ((item + 1) * self.batch_size) % data_len\n",
    "\n",
    "        if ((item + 1) * self.batch_size) == data_len:\n",
    "            stop_idx = data_len\n",
    "\n",
    "        if stop_idx > start_idx:\n",
    "            idxs = idxs[start_idx:stop_idx]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "        open_arr = np_data[idxs]\n",
    "\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def get_data_from_array(self, open_array):\n",
    "        data = []\n",
    "        fnames = []\n",
    "        idxs = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in open_array:\n",
    "            fn_name = self.files[idx]\n",
    "\n",
    "            numpy_array = np.load(fn_name)\n",
    "\n",
    "            data.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            if self.label is not None:\n",
    "                labels.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            fnames.append(self.files[idx])\n",
    "            idxs.append(idx)\n",
    "\n",
    "        ret_dict = {'data': data, 'fnames': fnames, 'idxs': idxs}\n",
    "        if self.label is not None:\n",
    "            ret_dict['seg'] = labels\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "class WrappedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.is_indexable = False\n",
    "        if hasattr(self.dataset, \"__getitem__\") and not (hasattr(self.dataset, \"use_next\") and self.dataset.use_next is True):\n",
    "            self.is_indexable = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if not self.is_indexable:\n",
    "            item = next(self.dataset)\n",
    "        else:\n",
    "            item = self.dataset[index]\n",
    "        # item = self.transform(**item)\n",
    "        print(type(item))\n",
    "        old_data=item['data']\n",
    "        old_seg=item['seg']\n",
    "        \n",
    "        new_shape=(128,128,128)\n",
    "        result_list=[]\n",
    "        \n",
    "        for i in range(len(old_data)):\n",
    "            result_element = np.zeros(new_shape, dtype=old_data[i].dtype)\n",
    "            result_element= resize(old_data[i].astype(float), new_shape, order=3, clip=True, anti_aliasing=False)\n",
    "            result_list.append(result_element)\n",
    "        item['data']=result_list\n",
    "        result_list=[]\n",
    "        result_element = np.zeros(new_shape, dtype=old_seg[0].dtype)\n",
    "        unique_labels = np.unique(old_seg[0])\n",
    "        for i, c in enumerate(unique_labels):\n",
    "            mask = old_seg[0] == c\n",
    "            reshaped_multihot = resize(mask.astype(float), new_shape, order=1, mode=\"edge\", clip=True, anti_aliasing=False)\n",
    "            result_element[reshaped_multihot >= 0.5] = c\n",
    "        \n",
    "        result_list.append(result_element)\n",
    "        item['seg']=result_list\n",
    "        print(np.unique(result_list[0]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.dataset.num_batches)\n",
    "\n",
    "\n",
    "class MultiThreadedDataLoader(object):\n",
    "    def __init__(self, data_loader,  num_processes,transform=None, **kwargs):\n",
    "\n",
    "        self.cntr = 1\n",
    "        self.ds_wrapper = WrappedDataset(data_loader, transform)\n",
    "\n",
    "        self.generator = DataLoader(self.ds_wrapper, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                                    num_workers=num_processes, pin_memory=True, drop_last=False,\n",
    "                                    worker_init_fn=self.get_worker_init_fn())\n",
    "\n",
    "        self.num_processes = num_processes\n",
    "        self.iter = None\n",
    "\n",
    "    def get_worker_init_fn(self):\n",
    "        def init_fn(worker_id):\n",
    "            set_seed(worker_id + self.cntr)\n",
    "\n",
    "        return init_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.kill_iterator()\n",
    "        self.iter = iter(self.generator)\n",
    "        return self.iter\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter is None:\n",
    "            self.iter = iter(self.generator)\n",
    "        return next(self.iter)\n",
    "\n",
    "    def renew(self):\n",
    "        self.cntr += 1\n",
    "        self.kill_iterator()\n",
    "        self.generator.worker_init_fn = self.get_worker_init_fn()\n",
    "        self.iter = iter(self.generator)\n",
    "\n",
    "    def kill_iterator(self):\n",
    "        try:\n",
    "            if self.iter is not None:\n",
    "                self.iter._shutdown_workers()\n",
    "                for p in self.iter.workers:\n",
    "                    p.terminate()\n",
    "        except:\n",
    "            print(\"Could not kill Dataloader Iterator\")\n",
    "\n",
    "class NumpyDataSet(object):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000, seed=None, num_processes=8, num_cached_per_queue=8 * 4, target_size=128,\n",
    "                 file_pattern='*.npz', label=1, input=(0,), do_reshuffle=True, keys=None):#8*4->2*4  8->2\n",
    "\n",
    "        data_loader = NumpyDataLoader(base_dir=base_dir, mode=mode, batch_size=batch_size, num_batches=num_batches, seed=seed, file_pattern=file_pattern,\n",
    "                                      input=input, label=label, keys=keys)\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.do_reshuffle = do_reshuffle\n",
    "        self.number_of_slices = 1\n",
    "\n",
    "        self.transforms = None\n",
    "        self.augmenter = MultiThreadedDataLoader(data_loader, num_processes,num_cached_per_queue=num_cached_per_queue, seeds=seed,\n",
    "                                                 shuffle=do_reshuffle)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.do_reshuffle:\n",
    "            self.data_loader.reshuffle()\n",
    "        self.augmenter.renew()\n",
    "        return self.augmenter\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.augmenter)\n",
    "\n",
    "class CrossentropyND(torch.nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    Network has to have NO NONLINEARITY!\n",
    "    \"\"\"\n",
    "    def forward(self, inp, target):\n",
    "        target = target.long()\n",
    "        num_classes = inp.size()[1]\n",
    "\n",
    "        i0 = 1\n",
    "        i1 = 2\n",
    "\n",
    "        while i1 < len(inp.shape): # this is ugly but torch only allows to transpose two axes at once\n",
    "            inp = inp.transpose(i0, i1)\n",
    "            i0 += 1\n",
    "            i1 += 1\n",
    "\n",
    "        inp = inp.contiguous()\n",
    "        inp = inp.view(-1, num_classes)\n",
    "\n",
    "        target = target.view(-1,)\n",
    "\n",
    "        return super(CrossentropyND, self).forward(inp, target)\n",
    "\n",
    "def softmax_helper(x):\n",
    "    rpt = [1 for _ in range(len(x.size()))]\n",
    "    rpt[1] = x.size(1)\n",
    "    x_max = x.max(1, keepdim=True)[0].repeat(*rpt)\n",
    "    e_x = torch.exp(x - x_max)\n",
    "    return e_x / e_x.sum(1, keepdim=True).repeat(*rpt)   \n",
    "\n",
    "class DC_and_CE_loss(nn.Module):\n",
    "    def __init__(self, soft_dice_kwargs, ce_kwargs, aggregate=\"sum\"):\n",
    "        super(DC_and_CE_loss, self).__init__()\n",
    "        self.aggregate = aggregate\n",
    "        self.ce = CrossentropyND(**ce_kwargs)\n",
    "        self.dc = SoftDiceLoss(apply_nonlin=softmax_helper, **soft_dice_kwargs)\n",
    "\n",
    "    def forward(self, net_output, target):\n",
    "        dc_loss = self.dc(net_output, target)\n",
    "        ce_loss = self.ce(net_output, target)\n",
    "        if self.aggregate == \"sum\":\n",
    "            result = ce_loss + dc_loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"nah son\") # reserved for other stuff (later)\n",
    "        return result\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1., apply_nonlin=None, batch_dice=False, do_bg=True, smooth_in_nom=True, background_weight=1, rebalance_weights=None):\n",
    "        \"\"\"\n",
    "        hahaa no documentation for you today\n",
    "        :param smooth:\n",
    "        :param apply_nonlin:\n",
    "        :param batch_dice:\n",
    "        :param do_bg:\n",
    "        :param smooth_in_nom:\n",
    "        :param background_weight:\n",
    "        :param rebalance_weights:\n",
    "        \"\"\"\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        if not do_bg:\n",
    "            assert background_weight == 1, \"if there is no bg, then set background weight to 1 you dummy\"\n",
    "        self.rebalance_weights = rebalance_weights\n",
    "        self.background_weight = background_weight\n",
    "        if smooth_in_nom:\n",
    "            self.smooth_in_nom = smooth\n",
    "        else:\n",
    "            self.smooth_in_nom = 0\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "        self.y_onehot = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "        shp_x = x.shape\n",
    "        shp_y = y.shape\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "        if len(shp_x) != len(shp_y):\n",
    "            y = y.view((shp_y[0], 1, *shp_y[1:]))\n",
    "        # now x and y should have shape (B, C, X, Y(, Z))) and (B, 1, X, Y(, Z))), respectively\n",
    "        y_onehot = torch.zeros(shp_x)\n",
    "        if x.device.type == \"cuda\":\n",
    "            y_onehot = y_onehot.cuda(x.device.index)\n",
    "        y_onehot.scatter_(1, y, 1)\n",
    "        if not self.do_bg:\n",
    "            x = x[:, 1:]\n",
    "            y_onehot = y_onehot[:, 1:]\n",
    "        if not self.batch_dice:\n",
    "            if self.background_weight != 1 or (self.rebalance_weights is not None):\n",
    "                raise NotImplementedError(\"nah son\")\n",
    "            l = soft_dice(x, y_onehot, self.smooth, self.smooth_in_nom)\n",
    "        else:\n",
    "            l = soft_dice_per_batch_2(x, y_onehot, self.smooth, self.smooth_in_nom,\n",
    "                                      background_weight=self.background_weight,\n",
    "                                      rebalance_weights=self.rebalance_weights)\n",
    "        return l\n",
    "\n",
    "\n",
    "def soft_dice_per_batch(net_output, gt, smooth=1., smooth_in_nom=1., background_weight=1):\n",
    "    axes = tuple([0] + list(range(2, len(net_output.size()))))\n",
    "    intersect = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    denom = sum_tensor(net_output + gt, axes, keepdim=False)\n",
    "    weights = torch.ones(intersect.shape)\n",
    "    weights[0] = background_weight\n",
    "    if net_output.device.type == \"cuda\":\n",
    "        weights = weights.cuda(net_output.device.index)\n",
    "    result = (- ((2 * intersect + smooth_in_nom) / (denom + smooth)) * weights).mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "def soft_dice_per_batch_2(net_output, gt, smooth=1., smooth_in_nom=1., background_weight=1, rebalance_weights=None):\n",
    "    if rebalance_weights is not None and len(rebalance_weights) != gt.shape[1]:\n",
    "        rebalance_weights = rebalance_weights[1:] # this is the case when use_bg=False\n",
    "    axes = tuple([0] + list(range(2, len(net_output.size()))))\n",
    "    tp = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    fn = sum_tensor((1 - net_output) * gt, axes, keepdim=False)\n",
    "    fp = sum_tensor(net_output * (1 - gt), axes, keepdim=False)\n",
    "    weights = torch.ones(tp.shape)\n",
    "    weights[0] = background_weight\n",
    "    if net_output.device.type == \"cuda\":\n",
    "        weights = weights.cuda(net_output.device.index)\n",
    "    if rebalance_weights is not None:\n",
    "        rebalance_weights = torch.from_numpy(rebalance_weights).float()\n",
    "        if net_output.device.type == \"cuda\":\n",
    "            rebalance_weights = rebalance_weights.cuda(net_output.device.index)\n",
    "        tp = tp * rebalance_weights\n",
    "        fn = fn * rebalance_weights\n",
    "    result = (- ((2 * tp + smooth_in_nom) / (2 * tp + fp + fn + smooth)) * weights).mean()\n",
    "    return result\n",
    "\n",
    "def sum_tensor(inp, axes, keepdim=False):\n",
    "    axes = np.unique(axes).astype(int)\n",
    "    if keepdim:\n",
    "        for ax in axes:\n",
    "            inp = inp.sum(int(ax), keepdim=True)\n",
    "    else:\n",
    "        for ax in sorted(axes, reverse=True):\n",
    "            inp = inp.sum(int(ax))\n",
    "    return inp\n",
    "\n",
    "def soft_dice(net_output, gt, smooth=1., smooth_in_nom=1.):\n",
    "    axes = tuple(range(2, len(net_output.size())))\n",
    "    intersect = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    denom = sum_tensor(net_output + gt, axes, keepdim=False)\n",
    "    result = (- ((2 * intersect + smooth_in_nom) / (denom + smooth))).mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class UNetExperiment3D(PytorchExperiment):\n",
    "    def get_vars_from_sys_argv():\n",
    "        import sys\n",
    "        import argparse\n",
    "\n",
    "        if len(sys.argv) > 1:\n",
    "\n",
    "            parser = argparse.ArgumentParser()\n",
    "\n",
    "            # parse just config keys\n",
    "            parser.add_argument(\"config_path\", type=str)\n",
    "            parser.add_argument(\"resume_path\", type=str)\n",
    "\n",
    "            # parse args\n",
    "            param, unknown = parser.parse_known_args()\n",
    "\n",
    "            if len(unknown) > 0:\n",
    "                warnings.warn(\"Called with unknown arguments: %s\" % unknown, RuntimeWarning)\n",
    "\n",
    "            # update dict\n",
    "            return param.get(\"config_path\"), param.get(\"resume_path\")\n",
    "    \n",
    "    def _config_raw_from_input(self,\n",
    "                               config=None,\n",
    "                               name=None,\n",
    "                               n_epochs=None,\n",
    "                               seed=None,\n",
    "                               append_rnd_to_name=False):\n",
    "        _config_raw = None\n",
    "        if isinstance(config, str):\n",
    "            _config_raw = Config(file_=config)\n",
    "        elif isinstance(config, (Config, dict)):\n",
    "            _config_raw = Config(config=config)\n",
    "        else:\n",
    "            _config_raw = Config()\n",
    "        if n_epochs is None and _config_raw.get(\"n_epochs\") is not None:\n",
    "            n_epochs = _config_raw[\"n_epochs\"]\n",
    "        elif n_epochs is None and _config_raw.get(\"n_epochs\") is None:\n",
    "            n_epochs = 0\n",
    "        _config_raw[\"n_epochs\"] = n_epochs\n",
    "\n",
    "        if seed is None and _config_raw.get('seed') is not None:\n",
    "            seed = _config_raw['seed']\n",
    "        elif seed is None and _config_raw.get('seed') is None:\n",
    "            random_data = os.urandom(4)\n",
    "            seed = int.from_bytes(random_data, byteorder=\"big\")\n",
    "        _config_raw['seed'] = seed\n",
    "\n",
    "        if name is None and _config_raw.get(\"name\") is not None:\n",
    "            name = _config_raw[\"name\"]\n",
    "        elif name is None and _config_raw.get(\"name\") is None:\n",
    "            name = \"experiment\"\n",
    "        if append_rnd_to_name:\n",
    "            rnd_str = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n",
    "            name += \"_\" + rnd_str\n",
    "        \n",
    "        _config_raw[\"name\"] = name\n",
    "\n",
    "        return _config_raw\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config=None,\n",
    "                 name=None,\n",
    "                 n_epochs=None,\n",
    "                 seed=None,\n",
    "                 base_dir=None,\n",
    "                 globs=None,\n",
    "                 resume=None,\n",
    "                 ignore_resume_config=False,\n",
    "                 resume_save_types=(\"model\", \"optimizer\", \"simple\", \"th_vars\", \"results\"),\n",
    "                 resume_reset_epochs=True,\n",
    "                 parse_sys_argv=False,\n",
    "                 checkpoint_to_cpu=True,\n",
    "                 save_checkpoint_every_epoch=1,\n",
    "                 explogger_kwargs=None,\n",
    "                 explogger_freq=1,\n",
    "                 loggers=None,\n",
    "                 append_rnd_to_name=False):\n",
    "        print(globs.get(\"__file__\"))\n",
    "        config_path_from_argv = None\n",
    "        if parse_sys_argv:\n",
    "            config_path_from_argv, resume_path_from_argv = get_vars_from_sys_argv()\n",
    "            if resume_path_from_argv:\n",
    "                resume = resume_path_from_argv\n",
    "\n",
    "        # construct _config_raw\n",
    "        if config_path_from_argv is None:\n",
    "            self._config_raw = self._config_raw_from_input(config, name, n_epochs, seed, append_rnd_to_name)\n",
    "            \n",
    "        else:\n",
    "            self._config_raw = Config(file_=config_path_from_argv)\n",
    "        print(self._config_raw)    \n",
    "        update_from_sys_argv(self._config_raw)\n",
    "        print(\"-----\")\n",
    "        \n",
    "        print(self._config_raw)\n",
    "        print(globs.get(\"__file__\"))\n",
    "        # set a few experiment attributes\n",
    "        self.n_epochs = self._config_raw[\"n_epochs\"]\n",
    "        self._seed = self._config_raw['seed']\n",
    "        set_seed(self._seed)\n",
    "        self.exp_name = self._config_raw[\"name\"]\n",
    "        self._checkpoint_to_cpu = checkpoint_to_cpu\n",
    "        self._save_checkpoint_every_epoch = save_checkpoint_every_epoch\n",
    "        self.results = dict()\n",
    "\n",
    "        # get base_dir from _config_raw or store there\n",
    "        if base_dir is not None:\n",
    "            self._config_raw[\"base_dir\"] = base_dir\n",
    "        base_dir = self._config_raw[\"base_dir\"]\n",
    "\n",
    "        # Construct experiment logger (automatically activated if base_dir is there)\n",
    "        self.loggers = {}\n",
    "        logger_list = []\n",
    "        if base_dir is not None:\n",
    "            if explogger_kwargs is None:\n",
    "                explogger_kwargs = {}\n",
    "            self.elog = PytorchExperimentLogger(base_dir=base_dir,\n",
    "                                                exp_name=self.exp_name,\n",
    "                                                **explogger_kwargs)\n",
    "            if explogger_freq is not None and explogger_freq > 0:\n",
    "                logger_list.append((self.elog, explogger_freq))\n",
    "            self.results = ResultLogDict(\"results-log.json\", base_dir=self.elog.result_dir)\n",
    "        else:\n",
    "            self.elog = None\n",
    "        \n",
    "        print(\"===\")\n",
    "        print(self.elog.work_dir)\n",
    "        print(self.elog.save_dir)\n",
    "        self._resume_path = None\n",
    "        self._resume_save_types = resume_save_types\n",
    "        self._ignore_resume_config = ignore_resume_config\n",
    "        self._resume_reset_epochs = resume_reset_epochs\n",
    "        if resume is not None:\n",
    "            if isinstance(resume, str):\n",
    "                if resume == \"last\":\n",
    "                    if base_dir is None:\n",
    "                        raise ValueError(\"resume='last' requires base_dir.\")\n",
    "                    self._resume_path = os.path.join(base_dir, sorted(os.listdir(base_dir))[-1])\n",
    "                else:\n",
    "                    self._resume_path = resume\n",
    "            elif isinstance(resume, PytorchExperiment):\n",
    "                self._resume_path = resume.elog.base_dir\n",
    "        \n",
    "        if self._resume_path is not None and not self._ignore_resume_config:\n",
    "            self._config_raw.update(Config(file_=os.path.join(self._resume_path, \"config\", \"config.json\")),\n",
    "                                    ignore=list(map(lambda x: re.sub(\"^-+\", \"\", x), sys.argv)))\n",
    "        print(self.elog.save_dir)\n",
    "        print(type(globs))\n",
    "        print(globs.get(\"__file__\"))\n",
    "        print(\"here\")\n",
    "        \n",
    "        PytorchExperiment.__init__(self,config=config,\n",
    "                 name=name,\n",
    "                 n_epochs=n_epochs,\n",
    "                 seed=seed,\n",
    "                 base_dir=base_dir,\n",
    "                 globs=globs,\n",
    "                 resume=None,\n",
    "                 ignore_resume_config=False,\n",
    "                 resume_save_types=(\"model\", \"optimizer\", \"simple\", \"th_vars\", \"results\"),\n",
    "                 resume_reset_epochs=True,\n",
    "                 parse_sys_argv=True,#f->t\n",
    "                 checkpoint_to_cpu=True,\n",
    "                 save_checkpoint_every_epoch=1,\n",
    "                 explogger_kwargs=None,\n",
    "                 explogger_freq=1,\n",
    "                 loggers=loggers,\n",
    "                 append_rnd_to_name=False)\n",
    "        \n",
    "        \n",
    "    def setup(self):\n",
    "        data_dir='/home/jovyan/main/BraTS2020_TrainingData/'\n",
    "        with open(os.path.join(data_dir, \"splits.pkl\"), 'rb') as f:\n",
    "          splits = pickle.load(f)\n",
    "        tr_keys = splits[0]['train']\n",
    "        val_keys = splits[0]['val']\n",
    "        test_keys = splits[0]['test']\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_data_loader = NumpyDataSet(data_dir, target_size=64, batch_size=8,keys=tr_keys)\n",
    "        self.val_data_loader = NumpyDataSet(data_dir, target_size=64, batch_size=8,\n",
    "                                            keys=val_keys, mode=\"val\", do_reshuffle=False)\n",
    "        self.model = UNet3D(num_classes=3, in_channels=1)\n",
    "        self.model.to(self.device)\n",
    "        self.loss = DC_and_CE_loss({'batch_dice': True, 'smooth': 1e-5, 'smooth_in_nom': True,\n",
    "                                    'do_bg': False, 'rebalance_weights': None, 'background_weight': 1}, OrderedDict())\n",
    "        print(\"loss ok\")\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # If directory for checkpoint is provided, we load it.\n",
    "        if self.config.do_load_checkpoint:\n",
    "            if self.config.checkpoint_dir == '':\n",
    "                print('checkpoint_dir is empty, please provide directory to load checkpoint.')\n",
    "            else:\n",
    "                self.load_checkpoint(name=self.config.checkpoint_dir, save_types=(\"model\",))\n",
    "\n",
    "        self.save_checkpoint(name=\"checkpoint_start\")\n",
    "        \n",
    "        self.elog.print('Experiment set up.')\n",
    "        print(\"set up ok\")\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        self.elog.print('=====TRAIN=====')\n",
    "        self.model.train()\n",
    "\n",
    "        batch_counter = 0\n",
    "        for data_batch in self.train_data_loader:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Shape of data_batch = [1, b, c, w, h]\n",
    "            # Desired shape = [b, c, w, h]\n",
    "            # Move data and target to the GPU\n",
    "            data = data_batch['data'][0].float().to(self.device)\n",
    "            target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "            pred = self.model(data)\n",
    "\n",
    "            loss = self.loss(pred, target.squeeze())\n",
    "            # loss = self.ce_loss(pred, target.squeeze())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Some logging and plotting\n",
    "            if (batch_counter % self.config.plot_freq) == 0:\n",
    "                self.elog.print('Epoch: %d Loss: %.4f' % (self._epoch_idx, loss))\n",
    "\n",
    "                self.add_result(value=loss.item(), name='Train_Loss', tag='Loss', counter=epoch + (batch_counter / self.train_data_loader.data_loader.num_batches))\n",
    "\n",
    "                self.clog.show_image_grid(data[:,:,30].float(), name=\"data\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "                self.clog.show_image_grid(target[:,:,30].float(), name=\"mask\", title=\"Mask\", n_iter=epoch)\n",
    "                self.clog.show_image_grid(torch.argmax(pred.cpu(), dim=1, keepdim=True)[:,:,30], name=\"unt_argmax\", title=\"Unet\", n_iter=epoch)\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        print(\"----validate------\")\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        self.elog.print('VALIDATE')\n",
    "        self.model.eval()\n",
    "\n",
    "        data = None\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_batch in self.val_data_loader:\n",
    "                data = data_batch['data'][0].float().to(self.device)\n",
    "                target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "                pred = self.model(data)\n",
    "\n",
    "                loss = self.loss(pred, target.squeeze())\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        assert data is not None, 'data is None. Please check if your dataloader works properly'\n",
    "        self.scheduler.step(np.mean(loss_list))\n",
    "\n",
    "        self.elog.print('Epoch: %d Loss: %.4f' % (self._epoch_idx, float(np.mean(loss_list))))\n",
    "\n",
    "        self.add_result(value=np.mean(loss_list), name='Val_Loss', tag='Loss', counter=epoch+1)\n",
    "\n",
    "        self.clog.show_image_grid(data[:,:,30].float(), name=\"data_val\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "        self.clog.show_image_grid(target[:,:,30].float(), name=\"mask_val\", title=\"Mask\", n_iter=epoch)\n",
    "        self.clog.show_image_grid(torch.argmax(pred.data.cpu()[:,:,30], dim=1, keepdim=True), name=\"unt_argmax_val\", title=\"Unet\", n_iter=epoch)\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    c = get_config()\n",
    "    print(globals().get(\"__file__\"))\n",
    "    exp = UNetExperiment3D(config=c, name=c.name, n_epochs=c.n_epochs,\n",
    "                             seed=42, append_rnd_to_name=c.append_rnd_string, globs=globals(),\n",
    "                             # visdomlogger_kwargs={\"auto_start\": c.start_visdom},\n",
    "                             loggers={\n",
    "                                 \"visdom\": (\"visdom\", {\"auto_start\": c.start_visdom})\n",
    "                             }\n",
    "                             )\n",
    "\n",
    "    exp.run()\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 137, 167, 133)\n",
    "# (5, 143, 176, 131)\n",
    "# (5, 137, 167, 124)\n",
    "# (5, 143, 187, 138)\n",
    "# (5, 144, 170, 138)\n",
    "# (5, 140, 186, 136)\n",
    "# (5, 146, 160, 127)\n",
    "# (5, 139, 158, 137)\n",
    "# (5, 145, 172, 140)\n",
    "# (5, 140, 173, 130)\n",
    "# (5, 140, 164, 145)\n",
    "# (5, 140, 182, 132)\n",
    "# (5, 144, 168, 146)\n",
    "# (5, 141, 178, 135)\n",
    "# (5, 145, 177, 140)\n",
    "# (5, 147, 167, 125)\n",
    "# (5, 138, 167, 142)\n",
    "# (5, 146, 178, 139)\n",
    "# (5, 136, 157, 133)\n",
    "# (5, 140, 187, 137)\n",
    "# (5, 137, 174, 139)\n",
    "# (5, 137, 166, 140)\n",
    "# (5, 141, 177, 140)\n",
    "# (5, 137, 169, 138)\n",
    "# (5, 143, 174, 137)\n",
    "# (5, 141, 178, 140)\n",
    "# (5, 143, 187, 132)\n",
    "# (5, 141, 174, 138)\n",
    "# (5, 136, 173, 131)\n",
    "# (5, 136, 168, 134)\n",
    "# (5, 141, 171, 130)\n",
    "# (5, 135, 163, 129)\n",
    "# (5, 138, 168, 128)\n",
    "# (5, 149, 176, 143)\n",
    "# (5, 138, 179, 140)\n",
    "# (5, 138, 167, 135)\n",
    "# (5, 141, 176, 144)\n",
    "# (5, 134, 157, 126)\n",
    "# (5, 142, 184, 141)\n",
    "# (5, 129, 175, 128)\n",
    "# (5, 144, 170, 130)\n",
    "# (5, 144, 173, 137)\n",
    "# (5, 130, 167, 148)\n",
    "# (5, 135, 162, 142)\n",
    "# (5, 140, 176, 133)\n",
    "# (5, 142, 185, 132)\n",
    "# (5, 141, 165, 143)\n",
    "# (5, 141, 173, 131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import pilutil\n",
    "\n",
    "x = np.zeros((255, 255), dtype=np.uint8)\n",
    "x[:] = np.arange(255)\n",
    "pilutil.imsave('gradient.png', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch multi processing\n"
     ]
    }
   ],
   "source": [
    "from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from slackclient import SlackClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.misc.pilutil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53/2797739665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpilutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimsave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy.misc.pilutil'"
     ]
    }
   ],
   "source": [
    "from scipy.misc.pilutil import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_vq' from partially initialized module 'scipy.cluster' (most likely due to a circular import) (/opt/conda/lib/python3.9/site-packages/scipy/cluster/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/2668183092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinedLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/logger/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinedlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinedLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpyseabornplotlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpySeabornPlotLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpyplotfilelogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyPlotFileLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextfilelogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextFileLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/logger/plt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpyseabornplotlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpySeabornPlotLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/logger/plt/numpyseabornplotlogger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmiscplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hierarchy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0m__docformat__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'restructuredtext'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_vq' from partially initialized module 'scipy.cluster' (most likely due to a circular import) (/opt/conda/lib/python3.9/site-packages/scipy/cluster/__init__.py)"
     ]
    }
   ],
   "source": [
    "from trixi.logger import CombinedLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
