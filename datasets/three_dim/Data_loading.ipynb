{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "av9RCfswtnhN",
    "outputId": "d350c391-7977-4cc5-d1d0-b9a5e5a6235a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn==0.4.2 in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.4.2) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 uninstall scikit-learn\n",
    "# !pip3 install scikit-learn==0.24.2\n",
    "# !pip3 uninstall imbalanced-learn==0.5.0\n",
    "!pip3 install imbalanced-learn==0.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ydCZ2vYmsmfO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "781d0afc-3254-4a90-a749-79a4c4d6df31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trixi in /usr/local/lib/python3.7/dist-packages (0.1.2.2)\n",
      "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.16.0)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.10.0+cu111)\n",
      "Requirement already satisfied: scikit-learn==0.20.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.20.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.4.1)\n",
      "Requirement already satisfied: python-telegram-bot>=10.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (13.11)\n",
      "Requirement already satisfied: pathos>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.2.8)\n",
      "Requirement already satisfied: portalocker>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.21.5)\n",
      "Requirement already satisfied: visdom>=0.1.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.1.8.9)\n",
      "Requirement already satisfied: graphviz>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (3.2.2)\n",
      "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.3.0)\n",
      "Requirement already satisfied: plotly>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (5.5.0)\n",
      "Requirement already satisfied: Flask>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.1.4)\n",
      "Requirement already satisfied: Pillow>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from trixi) (9.0.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.1+cu111)\n",
      "Requirement already satisfied: slackclient>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (2.9.3)\n",
      "Requirement already satisfied: seaborn>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.11.2)\n",
      "Requirement already satisfied: tb-nightly==1.14.0a20190523 in /usr/local/lib/python3.7/dist-packages (from trixi) (1.14.0a20190523)\n",
      "Requirement already satisfied: umap-learn>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from trixi) (0.4.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.43.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (3.17.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly==1.14.0a20190523->trixi) (1.0.1)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (7.1.2)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.12.2->trixi) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.12.2->trixi) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly==1.14.0a20190523->trixi) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.2->trixi) (1.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (0.3.0)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.7/dist-packages (from pathos>=0.2.0->trixi) (1.6.6.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=2.5.1->trixi) (8.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (6.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2021.10.8)\n",
      "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (3.6.3)\n",
      "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot>=10.1.0->trixi) (2018.9)\n",
      "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (57.4.0)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot>=10.1.0->trixi) (1.5.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.8.1->trixi) (1.3.5)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>3.5.2 in /usr/local/lib/python3.7/dist-packages (from slackclient>=1.3.1->trixi) (3.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.0.11)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (1.7.2)\n",
      "Requirement already satisfied: numba!=0.47,>=0.46 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.3.6->trixi) (0.51.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba!=0.47,>=0.46->umap-learn>=0.3.6->trixi) (0.34.0)\n",
      "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (0.1.0)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (2.23.0)\n",
      "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (1.32)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.4->trixi) (22.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>3.5.2->slackclient>=1.3.1->trixi) (2.10)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom>=0.1.8.4->trixi) (2.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.4->trixi) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install trixi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j2S22Nyy9-Ll"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/main/networks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0a20190523\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import version\n",
    "print(version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHXu2WlBqJ3i",
    "outputId": "9f4bc06c-09a8-447e-907b-1d681d1393e2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"append_rnd_string\": false,\n",
      "    \"author\": \"tinawytt\",\n",
      "    \"base_dir\": \"/home/jovyan/main/\",\n",
      "    \"batch_size\": 8,\n",
      "    \"checkpoint_dir\": \"\",\n",
      "    \"data_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_root_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"data_test_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"device\": \"cuda\",\n",
      "    \"do_instancenorm\": true,\n",
      "    \"do_load_checkpoint\": false,\n",
      "    \"fold\": 0,\n",
      "    \"in_channels\": 1,\n",
      "    \"learning_rate\": 0.0002,\n",
      "    \"model_dir\": \"/home/jovyan/main/\",\n",
      "    \"n_epochs\": 10,\n",
      "    \"name\": \"Basic_Unet\",\n",
      "    \"num_classes\": 3,\n",
      "    \"patch_size\": 64,\n",
      "    \"plot_freq\": 10,\n",
      "    \"split_dir\": \"/home/jovyan/main/BraTS2020_TrainingData/\",\n",
      "    \"start_visdom\": true\n",
      "}\n",
      "It's Alive!\n",
      "You can navigate to http://jupyter-yw016:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/visdom/server.py\", line 1803, in start_server\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 586, in run_forever\n",
      "    self._check_running()\n",
      "  File \"/opt/conda/lib/python3.9/asyncio/base_events.py\", line 578, in _check_running\n",
      "    raise RuntimeError('This event loop is already running')\n",
      "RuntimeError: This event loop is already running\n",
      "Setting up a new session...\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 174, in _new_conn\n",
      "    conn = connection.create_connection(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/connection.py\", line 95, in create_connection\n",
      "    raise err\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\n",
      "    httplib_response = self._make_request(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 398, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 239, in request\n",
      "    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1279, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1325, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1274, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 1034, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/opt/conda/lib/python3.9/http/client.py\", line 974, in send\n",
      "    self.connect()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 205, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connection.py\", line 186, in _new_conn\n",
      "    raise NewConnectionError(\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f6361b7bbb0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/adapters.py\", line 440, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 785, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /env/Basic_Unet (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6361b7bbb0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/visdom/__init__.py\", line 708, in _send\n",
      "    return self._handle_post(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 577, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 529, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/sessions.py\", line 645, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /env/Basic_Unet (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6361b7bbb0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "[Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Visdom on Port: 8080\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51/2678486526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m exp = UNetExperiment3D(config=c, name=c.name, n_epochs=c.n_epochs,\n\u001b[0m\u001b[1;32m    584\u001b[0m                          \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_rnd_to_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_rnd_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                          \u001b[0;31m# visdomlogger_kwargs={\"auto_start\": c.start_visdom},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/experiment/pytorchexperiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, name, n_epochs, seed, base_dir, globs, resume, ignore_resume_config, resume_save_types, resume_reset_epochs, parse_sys_argv, checkpoint_to_cpu, save_checkpoint_every_epoch, explogger_kwargs, explogger_freq, loggers, append_rnd_to_name)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resume_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore_resume_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             self._config_raw.update(Config(file_=os.path.join(self._resume_path, \"config\", \"config.json\")),\n\u001b[0;32m--> 267\u001b[0;31m                                     ignore=list(map(lambda x: re.sub(\"^-+\", \"\", x), sys.argv)))\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# Save everything we need to reproduce experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/util/sourcepacker.py\u001b[0m in \u001b[0;36mzip_sources\u001b[0;34m(globs, filename)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpy_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourcePacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_sources_and_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mrepo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourcePacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__file__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/util/sourcepacker.py\u001b[0m in \u001b[0;36mgit_info\u001b[0;34m(file_)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mold_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/posixpath.py\u001b[0m in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;34m\"\"\"Return an absolute path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import random\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage.transform import resize\n",
    "from trixi.util.pytorchutils import set_seed\n",
    "import numpy as np\n",
    "import pickle\n",
    "from UNet3D import UNet3D\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import OrderedDict\n",
    "# from networks.UNet3D import UNet3D\n",
    "from trixi.util import Config\n",
    "from trixi.experiment.pytorchexperiment import PytorchExperiment\n",
    "from torch import nn\n",
    "\n",
    "def get_config():\n",
    "    # Set your own path, if needed.\n",
    "    data_root_dir = '/home/jovyan/main/BraTS2020_TrainingData/'  # The path where the downloaded dataset is stored.\n",
    "\n",
    "    c = Config(\n",
    "        update_from_argv=True,  # If set 'True', it allows to update each configuration by a cmd/terminal parameter.\n",
    "\n",
    "        # Train parameters\n",
    "        num_classes=3,\n",
    "        in_channels=1,\n",
    "        batch_size=8,\n",
    "        patch_size=64,\n",
    "        n_epochs=10,\n",
    "        learning_rate=0.0002,\n",
    "        fold=0,  # The 'splits.pkl' may contain multiple folds. Here we choose which one we want to use.\n",
    "\n",
    "        device=\"cuda\",  # 'cuda' is the default CUDA device, you can use also 'cpu'. For more information, see https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "        # Logging parameters\n",
    "        name='Basic_Unet',\n",
    "        author='tinawytt',  # Author of this project\n",
    "        plot_freq=10,  # How often should stuff be shown in visdom\n",
    "        append_rnd_string=False,  # Appends a random string to the experiment name to make it unique.\n",
    "        start_visdom=True,  # You can either start a visom server manually or have trixi start it for you.\n",
    "\n",
    "        do_instancenorm=True,  # Defines whether or not the UNet does a instance normalization in the contracting path\n",
    "        do_load_checkpoint=False,\n",
    "        checkpoint_dir='',\n",
    "\n",
    "        \n",
    "        base_dir='/home/jovyan/main/',  # Where to log the output of the experiment.\n",
    "\n",
    "        data_root_dir=data_root_dir,  # The path where the downloaded dataset is stored.\n",
    "        data_dir=data_root_dir,  # This is where your training and validation data is stored\n",
    "        data_test_dir=data_root_dir,  # This is where your test data is stored\n",
    "\n",
    "        split_dir=data_root_dir,  # This is where the 'splits.pkl' file is located, that holds your splits.\n",
    "\n",
    "        # execute a segmentation process on a specific image using the model\n",
    "        model_dir=os.path.join('/home/jovyan/main/', ''),  # the model being used for segmentation\n",
    "    )\n",
    "\n",
    "    print(c)\n",
    "    return c\n",
    "\n",
    "def load_dataset(base_dir, pattern='*.npz', keys=None):\n",
    "    fls = []\n",
    "    files_len = []\n",
    "    dataset = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        i = 0\n",
    "        for filename in sorted(fnmatch.filter(files, pattern)):\n",
    "\n",
    "            if keys is not None and filename[:-4] in keys:\n",
    "                npz_file = os.path.join(root, filename)\n",
    "                numpy_array = np.load(npz_file)['data']\n",
    "                \n",
    "                fls.append(npz_file)\n",
    "                files_len.append(numpy_array.shape[1])\n",
    "\n",
    "                dataset.extend([i])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    return fls, files_len, dataset\n",
    "\n",
    "class SlimDataLoaderBase(object):\n",
    "    def __init__(self, data, batch_size, number_of_threads_in_multithreaded=None):\n",
    "        __metaclass__ = ABCMeta\n",
    "        self.number_of_threads_in_multithreaded = number_of_threads_in_multithreaded\n",
    "        self._data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.thread_id = 0\n",
    "\n",
    "    def set_thread_id(self, thread_id):\n",
    "        self.thread_id = thread_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.generate_train_batch()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_train_batch(self):\n",
    "        '''override this\n",
    "        Generate your batch from self._data .Make sure you generate the correct batch size (self.BATCH_SIZE)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "\n",
    "class NumpyDataLoader(SlimDataLoaderBase):\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000,\n",
    "                 seed=None, file_pattern='*.npz', label=1, input=(0,), keys=None):\n",
    "\n",
    "        shorter_keys=[]\n",
    "        for key in keys:\n",
    "            arr=key.split('/')\n",
    "            \n",
    "            shorter_keys.append(arr[len(arr)-1])\n",
    "        \n",
    "        keys=shorter_keys\n",
    "        self.files, self.file_len, self.dataset = load_dataset(base_dir=base_dir, pattern=file_pattern, keys=keys )\n",
    "        \n",
    "        super(NumpyDataLoader, self).__init__(self.dataset, batch_size, num_batches)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.use_next = False\n",
    "        if mode == \"train\":\n",
    "            self.use_next = False\n",
    "\n",
    "        self.idxs = list(range(0, len(self.dataset)))\n",
    "\n",
    "        self.data_len = len(self.dataset)\n",
    "\n",
    "        self.num_batches = min((self.data_len // self.batch_size)+10, num_batches)\n",
    "\n",
    "        if isinstance(label, int):\n",
    "            label = (label,)\n",
    "        self.input = input\n",
    "        self.label = label\n",
    "\n",
    "        self.np_data = np.asarray(self.dataset)\n",
    "\n",
    "    def reshuffle(self):\n",
    "        print(\"Reshuffle...\")\n",
    "        random.shuffle(self.idxs)\n",
    "        print(\"Initializing... this might take a while...\")\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        open_arr = random.sample(self._data, self.batch_size)\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        n_items = min(self.data_len // self.batch_size, self.num_batches)\n",
    "        return n_items\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        idxs = self.idxs\n",
    "        data_len = len(self.dataset)\n",
    "        np_data = self.np_data\n",
    "\n",
    "        if item > len(self):\n",
    "            raise StopIteration()\n",
    "        if (item * self.batch_size) == data_len:\n",
    "            raise StopIteration()\n",
    "\n",
    "        start_idx = (item * self.batch_size) % data_len\n",
    "        stop_idx = ((item + 1) * self.batch_size) % data_len\n",
    "\n",
    "        if ((item + 1) * self.batch_size) == data_len:\n",
    "            stop_idx = data_len\n",
    "\n",
    "        if stop_idx > start_idx:\n",
    "            idxs = idxs[start_idx:stop_idx]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "        open_arr = np_data[idxs]\n",
    "\n",
    "        return self.get_data_from_array(open_arr)\n",
    "\n",
    "    def get_data_from_array(self, open_array):\n",
    "        data = []\n",
    "        fnames = []\n",
    "        idxs = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in open_array:\n",
    "            fn_name = self.files[idx]\n",
    "\n",
    "            numpy_array = np.load(fn_name)\n",
    "\n",
    "            data.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            if self.label is not None:\n",
    "                labels.append(numpy_array[list(self.input)])   # 'None' keeps the dimension\n",
    "\n",
    "            fnames.append(self.files[idx])\n",
    "            idxs.append(idx)\n",
    "\n",
    "        ret_dict = {'data': data, 'fnames': fnames, 'idxs': idxs}\n",
    "        if self.label is not None:\n",
    "            ret_dict['seg'] = labels\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "class WrappedDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.transform = transform\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.is_indexable = False\n",
    "        if hasattr(self.dataset, \"__getitem__\") and not (hasattr(self.dataset, \"use_next\") and self.dataset.use_next is True):\n",
    "            self.is_indexable = True\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if not self.is_indexable:\n",
    "            item = next(self.dataset)\n",
    "        else:\n",
    "            item = self.dataset[index]\n",
    "        # item = self.transform(**item)\n",
    "        print(type(item))\n",
    "        old_data=item['data']\n",
    "        old_seg=item['seg']\n",
    "        \n",
    "        new_shape=(128,128,128)\n",
    "        result_list=[]\n",
    "        \n",
    "        for i in range(len(old_data)):\n",
    "            result_element = np.zeros(new_shape, dtype=old_data[i].dtype)\n",
    "            result_element= resize(old_data[i].astype(float), new_shape, order=3, clip=True, anti_aliasing=False)\n",
    "            result_list.append(result_element)\n",
    "        item['data']=result_list\n",
    "        result_list=[]\n",
    "        result_element = np.zeros(new_shape, dtype=old_seg[0].dtype)\n",
    "        unique_labels = np.unique(old_seg[0])\n",
    "        for i, c in enumerate(unique_labels):\n",
    "            mask = old_seg[0] == c\n",
    "            reshaped_multihot = resize(mask.astype(float), new_shape, order=1, mode=\"edge\", clip=True, anti_aliasing=False)\n",
    "            result_element[reshaped_multihot >= 0.5] = c\n",
    "        \n",
    "        result_list.append(result_element)\n",
    "        item['seg']=result_list\n",
    "        print(np.unique(result_list[0]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.dataset.num_batches)\n",
    "\n",
    "\n",
    "class MultiThreadedDataLoader(object):\n",
    "    def __init__(self, data_loader,  num_processes,transform=None, **kwargs):\n",
    "\n",
    "        self.cntr = 1\n",
    "        self.ds_wrapper = WrappedDataset(data_loader, transform)\n",
    "\n",
    "        self.generator = DataLoader(self.ds_wrapper, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                                    num_workers=num_processes, pin_memory=True, drop_last=False,\n",
    "                                    worker_init_fn=self.get_worker_init_fn())\n",
    "\n",
    "        self.num_processes = num_processes\n",
    "        self.iter = None\n",
    "\n",
    "    def get_worker_init_fn(self):\n",
    "        def init_fn(worker_id):\n",
    "            set_seed(worker_id + self.cntr)\n",
    "\n",
    "        return init_fn\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.kill_iterator()\n",
    "        self.iter = iter(self.generator)\n",
    "        return self.iter\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iter is None:\n",
    "            self.iter = iter(self.generator)\n",
    "        return next(self.iter)\n",
    "\n",
    "    def renew(self):\n",
    "        self.cntr += 1\n",
    "        self.kill_iterator()\n",
    "        self.generator.worker_init_fn = self.get_worker_init_fn()\n",
    "        self.iter = iter(self.generator)\n",
    "\n",
    "    def kill_iterator(self):\n",
    "        try:\n",
    "            if self.iter is not None:\n",
    "                self.iter._shutdown_workers()\n",
    "                for p in self.iter.workers:\n",
    "                    p.terminate()\n",
    "        except:\n",
    "            print(\"Could not kill Dataloader Iterator\")\n",
    "\n",
    "class NumpyDataSet(object):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dir, mode=\"train\", batch_size=16, num_batches=10000000, seed=None, num_processes=8, num_cached_per_queue=8 * 4, target_size=128,\n",
    "                 file_pattern='*.npz', label=1, input=(0,), do_reshuffle=True, keys=None):#8*4->2*4  8->2\n",
    "\n",
    "        data_loader = NumpyDataLoader(base_dir=base_dir, mode=mode, batch_size=batch_size, num_batches=num_batches, seed=seed, file_pattern=file_pattern,\n",
    "                                      input=input, label=label, keys=keys)\n",
    "\n",
    "        self.data_loader = data_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.do_reshuffle = do_reshuffle\n",
    "        self.number_of_slices = 1\n",
    "\n",
    "        self.transforms = None\n",
    "        self.augmenter = MultiThreadedDataLoader(data_loader, num_processes,num_cached_per_queue=num_cached_per_queue, seeds=seed,\n",
    "                                                 shuffle=do_reshuffle)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.do_reshuffle:\n",
    "            self.data_loader.reshuffle()\n",
    "        self.augmenter.renew()\n",
    "        return self.augmenter\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.augmenter)\n",
    "\n",
    "class CrossentropyND(torch.nn.CrossEntropyLoss):\n",
    "    \"\"\"\n",
    "    Network has to have NO NONLINEARITY!\n",
    "    \"\"\"\n",
    "    def forward(self, inp, target):\n",
    "        target = target.long()\n",
    "        num_classes = inp.size()[1]\n",
    "\n",
    "        i0 = 1\n",
    "        i1 = 2\n",
    "\n",
    "        while i1 < len(inp.shape): # this is ugly but torch only allows to transpose two axes at once\n",
    "            inp = inp.transpose(i0, i1)\n",
    "            i0 += 1\n",
    "            i1 += 1\n",
    "\n",
    "        inp = inp.contiguous()\n",
    "        inp = inp.view(-1, num_classes)\n",
    "\n",
    "        target = target.view(-1,)\n",
    "\n",
    "        return super(CrossentropyND, self).forward(inp, target)\n",
    "\n",
    "def softmax_helper(x):\n",
    "    rpt = [1 for _ in range(len(x.size()))]\n",
    "    rpt[1] = x.size(1)\n",
    "    x_max = x.max(1, keepdim=True)[0].repeat(*rpt)\n",
    "    e_x = torch.exp(x - x_max)\n",
    "    return e_x / e_x.sum(1, keepdim=True).repeat(*rpt)   \n",
    "\n",
    "class DC_and_CE_loss(nn.Module):\n",
    "    def __init__(self, soft_dice_kwargs, ce_kwargs, aggregate=\"sum\"):\n",
    "        super(DC_and_CE_loss, self).__init__()\n",
    "        self.aggregate = aggregate\n",
    "        self.ce = CrossentropyND(**ce_kwargs)\n",
    "        self.dc = SoftDiceLoss(apply_nonlin=softmax_helper, **soft_dice_kwargs)\n",
    "\n",
    "    def forward(self, net_output, target):\n",
    "        dc_loss = self.dc(net_output, target)\n",
    "        ce_loss = self.ce(net_output, target)\n",
    "        if self.aggregate == \"sum\":\n",
    "            result = ce_loss + dc_loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"nah son\") # reserved for other stuff (later)\n",
    "        return result\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1., apply_nonlin=None, batch_dice=False, do_bg=True, smooth_in_nom=True, background_weight=1, rebalance_weights=None):\n",
    "        \"\"\"\n",
    "        hahaa no documentation for you today\n",
    "        :param smooth:\n",
    "        :param apply_nonlin:\n",
    "        :param batch_dice:\n",
    "        :param do_bg:\n",
    "        :param smooth_in_nom:\n",
    "        :param background_weight:\n",
    "        :param rebalance_weights:\n",
    "        \"\"\"\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        if not do_bg:\n",
    "            assert background_weight == 1, \"if there is no bg, then set background weight to 1 you dummy\"\n",
    "        self.rebalance_weights = rebalance_weights\n",
    "        self.background_weight = background_weight\n",
    "        if smooth_in_nom:\n",
    "            self.smooth_in_nom = smooth\n",
    "        else:\n",
    "            self.smooth_in_nom = 0\n",
    "        self.do_bg = do_bg\n",
    "        self.batch_dice = batch_dice\n",
    "        self.apply_nonlin = apply_nonlin\n",
    "        self.smooth = smooth\n",
    "        self.y_onehot = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y = y.long()\n",
    "        shp_x = x.shape\n",
    "        shp_y = y.shape\n",
    "        if self.apply_nonlin is not None:\n",
    "            x = self.apply_nonlin(x)\n",
    "        if len(shp_x) != len(shp_y):\n",
    "            y = y.view((shp_y[0], 1, *shp_y[1:]))\n",
    "        # now x and y should have shape (B, C, X, Y(, Z))) and (B, 1, X, Y(, Z))), respectively\n",
    "        y_onehot = torch.zeros(shp_x)\n",
    "        if x.device.type == \"cuda\":\n",
    "            y_onehot = y_onehot.cuda(x.device.index)\n",
    "        y_onehot.scatter_(1, y, 1)\n",
    "        if not self.do_bg:\n",
    "            x = x[:, 1:]\n",
    "            y_onehot = y_onehot[:, 1:]\n",
    "        if not self.batch_dice:\n",
    "            if self.background_weight != 1 or (self.rebalance_weights is not None):\n",
    "                raise NotImplementedError(\"nah son\")\n",
    "            l = soft_dice(x, y_onehot, self.smooth, self.smooth_in_nom)\n",
    "        else:\n",
    "            l = soft_dice_per_batch_2(x, y_onehot, self.smooth, self.smooth_in_nom,\n",
    "                                      background_weight=self.background_weight,\n",
    "                                      rebalance_weights=self.rebalance_weights)\n",
    "        return l\n",
    "\n",
    "\n",
    "def soft_dice_per_batch(net_output, gt, smooth=1., smooth_in_nom=1., background_weight=1):\n",
    "    axes = tuple([0] + list(range(2, len(net_output.size()))))\n",
    "    intersect = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    denom = sum_tensor(net_output + gt, axes, keepdim=False)\n",
    "    weights = torch.ones(intersect.shape)\n",
    "    weights[0] = background_weight\n",
    "    if net_output.device.type == \"cuda\":\n",
    "        weights = weights.cuda(net_output.device.index)\n",
    "    result = (- ((2 * intersect + smooth_in_nom) / (denom + smooth)) * weights).mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "def soft_dice_per_batch_2(net_output, gt, smooth=1., smooth_in_nom=1., background_weight=1, rebalance_weights=None):\n",
    "    if rebalance_weights is not None and len(rebalance_weights) != gt.shape[1]:\n",
    "        rebalance_weights = rebalance_weights[1:] # this is the case when use_bg=False\n",
    "    axes = tuple([0] + list(range(2, len(net_output.size()))))\n",
    "    tp = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    fn = sum_tensor((1 - net_output) * gt, axes, keepdim=False)\n",
    "    fp = sum_tensor(net_output * (1 - gt), axes, keepdim=False)\n",
    "    weights = torch.ones(tp.shape)\n",
    "    weights[0] = background_weight\n",
    "    if net_output.device.type == \"cuda\":\n",
    "        weights = weights.cuda(net_output.device.index)\n",
    "    if rebalance_weights is not None:\n",
    "        rebalance_weights = torch.from_numpy(rebalance_weights).float()\n",
    "        if net_output.device.type == \"cuda\":\n",
    "            rebalance_weights = rebalance_weights.cuda(net_output.device.index)\n",
    "        tp = tp * rebalance_weights\n",
    "        fn = fn * rebalance_weights\n",
    "    result = (- ((2 * tp + smooth_in_nom) / (2 * tp + fp + fn + smooth)) * weights).mean()\n",
    "    return result\n",
    "\n",
    "def sum_tensor(inp, axes, keepdim=False):\n",
    "    axes = np.unique(axes).astype(int)\n",
    "    if keepdim:\n",
    "        for ax in axes:\n",
    "            inp = inp.sum(int(ax), keepdim=True)\n",
    "    else:\n",
    "        for ax in sorted(axes, reverse=True):\n",
    "            inp = inp.sum(int(ax))\n",
    "    return inp\n",
    "\n",
    "def soft_dice(net_output, gt, smooth=1., smooth_in_nom=1.):\n",
    "    axes = tuple(range(2, len(net_output.size())))\n",
    "    intersect = sum_tensor(net_output * gt, axes, keepdim=False)\n",
    "    denom = sum_tensor(net_output + gt, axes, keepdim=False)\n",
    "    result = (- ((2 * intersect + smooth_in_nom) / (denom + smooth))).mean()\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "class UNetExperiment3D(PytorchExperiment):\n",
    "    def setup(self):\n",
    "        data_dir='/home/jovyan/main/BraTS2020_TrainingData/'\n",
    "        with open(os.path.join(data_dir, \"splits.pkl\"), 'rb') as f:\n",
    "          splits = pickle.load(f)\n",
    "        tr_keys = splits[0]['train']\n",
    "        val_keys = splits[0]['val']\n",
    "        test_keys = splits[0]['test']\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_data_loader = NumpyDataSet(data_dir, target_size=64, batch_size=8,keys=tr_keys)\n",
    "        self.val_data_loader = NumpyDataSet(data_dir, target_size=64, batch_size=8,\n",
    "                                            keys=val_keys, mode=\"val\", do_reshuffle=False)\n",
    "        self.model = UNet3D(num_classes=3, in_channels=1)\n",
    "        self.model.to(self.device)\n",
    "        self.loss = DC_and_CE_loss({'batch_dice': True, 'smooth': 1e-5, 'smooth_in_nom': True,\n",
    "                                    'do_bg': False, 'rebalance_weights': None, 'background_weight': 1}, OrderedDict())\n",
    "        print(\"loss ok\")\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # If directory for checkpoint is provided, we load it.\n",
    "        if self.config.do_load_checkpoint:\n",
    "            if self.config.checkpoint_dir == '':\n",
    "                print('checkpoint_dir is empty, please provide directory to load checkpoint.')\n",
    "            else:\n",
    "                self.load_checkpoint(name=self.config.checkpoint_dir, save_types=(\"model\",))\n",
    "\n",
    "        self.save_checkpoint(name=\"checkpoint_start\")\n",
    "        \n",
    "        self.elog.print('Experiment set up.')\n",
    "        print(\"set up ok\")\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        self.elog.print('=====TRAIN=====')\n",
    "        self.model.train()\n",
    "\n",
    "        batch_counter = 0\n",
    "        for data_batch in self.train_data_loader:\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Shape of data_batch = [1, b, c, w, h]\n",
    "            # Desired shape = [b, c, w, h]\n",
    "            # Move data and target to the GPU\n",
    "            data = data_batch['data'][0].float().to(self.device)\n",
    "            target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "            pred = self.model(data)\n",
    "\n",
    "            loss = self.loss(pred, target.squeeze())\n",
    "            # loss = self.ce_loss(pred, target.squeeze())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Some logging and plotting\n",
    "            if (batch_counter % self.config.plot_freq) == 0:\n",
    "                self.elog.print('Epoch: %d Loss: %.4f' % (self._epoch_idx, loss))\n",
    "\n",
    "                self.add_result(value=loss.item(), name='Train_Loss', tag='Loss', counter=epoch + (batch_counter / self.train_data_loader.data_loader.num_batches))\n",
    "\n",
    "                self.clog.show_image_grid(data[:,:,30].float(), name=\"data\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "                self.clog.show_image_grid(target[:,:,30].float(), name=\"mask\", title=\"Mask\", n_iter=epoch)\n",
    "                self.clog.show_image_grid(torch.argmax(pred.cpu(), dim=1, keepdim=True)[:,:,30], name=\"unt_argmax\", title=\"Unet\", n_iter=epoch)\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        print(\"----validate------\")\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        self.elog.print('VALIDATE')\n",
    "        self.model.eval()\n",
    "\n",
    "        data = None\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_batch in self.val_data_loader:\n",
    "                data = data_batch['data'][0].float().to(self.device)\n",
    "                target = data_batch['seg'][0].long().to(self.device)\n",
    "\n",
    "                pred = self.model(data)\n",
    "\n",
    "                loss = self.loss(pred, target.squeeze())\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        assert data is not None, 'data is None. Please check if your dataloader works properly'\n",
    "        self.scheduler.step(np.mean(loss_list))\n",
    "\n",
    "        self.elog.print('Epoch: %d Loss: %.4f' % (self._epoch_idx, float(np.mean(loss_list))))\n",
    "\n",
    "        self.add_result(value=np.mean(loss_list), name='Val_Loss', tag='Loss', counter=epoch+1)\n",
    "\n",
    "        self.clog.show_image_grid(data[:,:,30].float(), name=\"data_val\", normalize=True, scale_each=True, n_iter=epoch)\n",
    "        self.clog.show_image_grid(target[:,:,30].float(), name=\"mask_val\", title=\"Mask\", n_iter=epoch)\n",
    "        self.clog.show_image_grid(torch.argmax(pred.data.cpu()[:,:,30], dim=1, keepdim=True), name=\"unt_argmax_val\", title=\"Unet\", n_iter=epoch)\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "\n",
    "c = get_config()\n",
    "exp = UNetExperiment3D(config=c, name=c.name, n_epochs=c.n_epochs,\n",
    "                         seed=42, append_rnd_to_name=c.append_rnd_string, globs=globals(),\n",
    "                         # visdomlogger_kwargs={\"auto_start\": c.start_visdom},\n",
    "                         loggers={\n",
    "                             \"visdom\": (\"visdom\", {\"auto_start\": c.start_visdom})\n",
    "                         }\n",
    "                         )\n",
    "\n",
    "exp.run()\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 137, 167, 133)\n",
    "# (5, 143, 176, 131)\n",
    "# (5, 137, 167, 124)\n",
    "# (5, 143, 187, 138)\n",
    "# (5, 144, 170, 138)\n",
    "# (5, 140, 186, 136)\n",
    "# (5, 146, 160, 127)\n",
    "# (5, 139, 158, 137)\n",
    "# (5, 145, 172, 140)\n",
    "# (5, 140, 173, 130)\n",
    "# (5, 140, 164, 145)\n",
    "# (5, 140, 182, 132)\n",
    "# (5, 144, 168, 146)\n",
    "# (5, 141, 178, 135)\n",
    "# (5, 145, 177, 140)\n",
    "# (5, 147, 167, 125)\n",
    "# (5, 138, 167, 142)\n",
    "# (5, 146, 178, 139)\n",
    "# (5, 136, 157, 133)\n",
    "# (5, 140, 187, 137)\n",
    "# (5, 137, 174, 139)\n",
    "# (5, 137, 166, 140)\n",
    "# (5, 141, 177, 140)\n",
    "# (5, 137, 169, 138)\n",
    "# (5, 143, 174, 137)\n",
    "# (5, 141, 178, 140)\n",
    "# (5, 143, 187, 132)\n",
    "# (5, 141, 174, 138)\n",
    "# (5, 136, 173, 131)\n",
    "# (5, 136, 168, 134)\n",
    "# (5, 141, 171, 130)\n",
    "# (5, 135, 163, 129)\n",
    "# (5, 138, 168, 128)\n",
    "# (5, 149, 176, 143)\n",
    "# (5, 138, 179, 140)\n",
    "# (5, 138, 167, 135)\n",
    "# (5, 141, 176, 144)\n",
    "# (5, 134, 157, 126)\n",
    "# (5, 142, 184, 141)\n",
    "# (5, 129, 175, 128)\n",
    "# (5, 144, 170, 130)\n",
    "# (5, 144, 173, 137)\n",
    "# (5, 130, 167, 148)\n",
    "# (5, 135, 162, 142)\n",
    "# (5, 140, 176, 133)\n",
    "# (5, 142, 185, 132)\n",
    "# (5, 141, 165, 143)\n",
    "# (5, 141, 173, 131)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import pilutil\n",
    "\n",
    "x = np.zeros((255, 255), dtype=np.uint8)\n",
    "x[:] = np.arange(255)\n",
    "pilutil.imsave('gradient.png', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch multi processing\n"
     ]
    }
   ],
   "source": [
    "from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from slackclient import SlackClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc.pilutil import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_vq' from partially initialized module 'scipy.cluster' (most likely due to a circular import) (/opt/conda/lib/python3.9/site-packages/scipy/cluster/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/2668183092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinedLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/logger/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinedlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCombinedLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpyseabornplotlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpySeabornPlotLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpyplotfilelogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyPlotFileLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextfilelogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextFileLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/logger/plt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpyseabornplotlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpySeabornPlotLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/trixi/logger/plt/numpyseabornplotlogger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrixi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmiscplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hierarchy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0m__docformat__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'restructuredtext'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_vq' from partially initialized module 'scipy.cluster' (most likely due to a circular import) (/opt/conda/lib/python3.9/site-packages/scipy/cluster/__init__.py)"
     ]
    }
   ],
   "source": [
    "from trixi.logger import CombinedLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Data_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
